{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7abtfSijy31lmEs/jnWiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeyScientist/Artificial-Intelligence/blob/main/Association.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "uR1-4vFLKAhS",
        "outputId": "e8b3bb13-482c-4f0f-e171-4f0df8f0368b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9851f6fe-7d11-46ff-9ce5-73ce30612850\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9851f6fe-7d11-46ff-9ce5-73ce30612850\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the data (assuming semicolon-separated values)\n",
        "df = pd.read_csv('single.csv', sep=';')\n",
        "\n",
        "# Step 2: Clean column names to handle spaces or special characters\n",
        "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
        "df.columns = df.columns.str.replace(r'[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters\n",
        "\n",
        "# Step 3: Print the cleaned column names to check if 'Country' exists\n",
        "print(\"Cleaned Column Names:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Step 4: Handle \"wrongly coded\" entries in 'Itemname'\n",
        "df['Itemname'] = df['Itemname'].replace(r'wrongly coded.*', 'Unknown Item', regex=True)\n",
        "\n",
        "# Step 5: Drop columns with completely empty values\n",
        "df = df.dropna(axis=1, how='all')\n",
        "\n",
        "# Step 6: Remove columns that start with 'Unnamed' (e.g., 'Country,,,')\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# Step 7: Clean 'Price' column: Replace commas with dots and convert to numeric\n",
        "df['Price'] = df['Price'].replace(',', '.', regex=True)  # Replace commas with dots\n",
        "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')  # Convert to numeric, coerce errors\n",
        "\n",
        "# Step 8: Clean 'Quantity' column: Convert to numeric\n",
        "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
        "\n",
        "# Step 9: Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Step 10: Handle missing values in 'CustomerID' and 'Country'\n",
        "# Ensure 'CustomerID' and 'Country' are available in cleaned columns\n",
        "if 'CustomerID' in df.columns:\n",
        "    df['CustomerID'] = df['CustomerID'].fillna('Unknown')  # Fill missing CustomerID with 'Unknown'\n",
        "else:\n",
        "    print(\"'CustomerID' column is missing!\")\n",
        "\n",
        "if 'Country' in df.columns:\n",
        "    df['Country'] = df['Country'].fillna('Unknown')  # Fill missing Country with 'Unknown'\n",
        "else:\n",
        "    print(\"'Country' column is missing!\")\n",
        "\n",
        "# Step 11: Show a snippet of the cleaned dataframe\n",
        "print(\"Cleaned DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 12: Check for any remaining \"Unknown Item\" entries (i.e., wrongly coded items)\n",
        "print(\"\\nRows with 'Unknown Item':\")\n",
        "print(df[df['Itemname'].str.contains('Unknown Item', na=False)])\n",
        "\n",
        "# Optional: Save the cleaned DataFrame to a new CSV file\n",
        "df.to_csv('cleaned_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN-wGasp1A49",
        "outputId": "901a8731-7136-47b9-e0e6-9f125d0c2bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Column Names:\n",
            "Index(['BillNo', 'Itemname', 'Quantity', 'Date', 'Price', 'CustomerID',\n",
            "       'Country'],\n",
            "      dtype='object')\n",
            "Cleaned DataFrame:\n",
            "   BillNo                             Itemname  Quantity                Date  \\\n",
            "0  536365   WHITE HANGING HEART T-LIGHT HOLDER       6.0 2010-01-12 08:26:00   \n",
            "1  536365                  WHITE METAL LANTERN       6.0 2010-01-12 08:26:00   \n",
            "2  536365       CREAM CUPID HEARTS COAT HANGER       8.0 2010-01-12 08:26:00   \n",
            "3  536365  KNITTED UNION FLAG HOT WATER BOTTLE       6.0 2010-01-12 08:26:00   \n",
            "4  536365       RED WOOLLY HOTTIE WHITE HEART.       6.0 2010-01-12 08:26:00   \n",
            "\n",
            "   Price CustomerID           Country  \n",
            "0   2.55    17850.0  United Kingdom,,  \n",
            "1   3.39    17850.0  United Kingdom,,  \n",
            "2   2.75    17850.0  United Kingdom,,  \n",
            "3   3.39    17850.0  United Kingdom,,  \n",
            "4   3.39    17850.0  United Kingdom,,  \n",
            "\n",
            "Rows with 'Unknown Item':\n",
            "        BillNo      Itemname  Quantity                Date  Price CustomerID  \\\n",
            "366291  569830  Unknown Item     800.0 2011-06-10 12:38:00    0.0    Unknown   \n",
            "366292  569831  Unknown Item    -800.0 2011-06-10 12:38:00    0.0    Unknown   \n",
            "406214  573114  Unknown Item    1000.0                 NaT    0.0    Unknown   \n",
            "406215  573115  Unknown Item   -1000.0                 NaT    0.0    Unknown   \n",
            "\n",
            "                  Country  \n",
            "366291  United Kingdom,,,  \n",
            "366292  United Kingdom,,,  \n",
            "406214  United Kingdom,,,  \n",
            "406215  United Kingdom,,,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def analyze_data_for_association_rules(df):\n",
        "    suggestions = {}\n",
        "\n",
        "    # Basic stats\n",
        "    num_rows, num_cols = df.shape\n",
        "    suggestions['rows'] = num_rows\n",
        "    suggestions['columns'] = num_cols\n",
        "\n",
        "    # Detect column types\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
        "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "    suggestions['categorical_features'] = categorical_cols\n",
        "    suggestions['numerical_features'] = numerical_cols\n",
        "\n",
        "    # Determine dimensions and rule types\n",
        "    rule_types = []\n",
        "    explanations = []\n",
        "\n",
        "    if num_cols == 1 and categorical_cols:\n",
        "        rule_types.append(\"Single-dimensional Association Rules\")\n",
        "        explanations.append(\"Only one categorical column detected. Rules can only be mined from within that column (e.g., frequent values in 'Itemname').\")\n",
        "\n",
        "    if len(categorical_cols) > 1:\n",
        "        rule_types.append(\"Multi-dimensional Association Rules\")\n",
        "        explanations.append(\"Multiple categorical columns found. Enables rules across dimensions (e.g., 'If Country is France and Customer is X, then likely buys Y').\")\n",
        "\n",
        "    if numerical_cols:\n",
        "        rule_types.append(\"Quantitative Association Rules\")\n",
        "        explanations.append(\"Numerical features detected. Enables mining based on numerical ranges (e.g., 'If Quantity > 10, then Price > 20').\")\n",
        "\n",
        "    # Detect boolean-like columns\n",
        "    boolean_like_cols = []\n",
        "    for col in categorical_cols + numerical_cols:\n",
        "        unique_vals = df[col].dropna().unique()\n",
        "        if set(unique_vals).issubset({0, 1, True, False}):\n",
        "            boolean_like_cols.append(col)\n",
        "\n",
        "    if boolean_like_cols:\n",
        "        rule_types.append(\"Boolean Association Rules\")\n",
        "        explanations.append(f\"Boolean-like columns found: {boolean_like_cols}. You can analyze co-occurrence or absence-based rules.\")\n",
        "\n",
        "    # Check for negative association candidates\n",
        "    negative_rule_candidates = []\n",
        "    if len(categorical_cols) >= 2:\n",
        "        for i, col1 in enumerate(categorical_cols):\n",
        "            for col2 in categorical_cols[i + 1:]:\n",
        "                cross_tab = pd.crosstab(df[col1], df[col2])\n",
        "                if (cross_tab == 0).sum().sum() > 0:\n",
        "                    negative_rule_candidates.append((col1, col2))\n",
        "\n",
        "    if negative_rule_candidates:\n",
        "        rule_types.append(\"Potential Negative Association Rules\")\n",
        "        explanations.append(\"Certain column pairs never or rarely occur together. May suggest exclusivity or negative relationships.\")\n",
        "\n",
        "    # Build suggestions with detail\n",
        "    suggestions['recommended_rule_types'] = {\n",
        "        \"types\": rule_types,\n",
        "        \"explanations\": explanations\n",
        "    }\n",
        "    suggestions['boolean_like_columns'] = boolean_like_cols\n",
        "    suggestions['negative_rule_candidates'] = negative_rule_candidates\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "\n",
        "# Example usage with your dataframe `df`\n",
        "results = analyze_data_for_association_rules(df)\n",
        "\n",
        "# Custom detailed printout\n",
        "print(\"📊 DATASET SUMMARY\")\n",
        "print(f\"- Rows: {results['rows']}\")\n",
        "print(f\"- Columns: {results['columns']}\")\n",
        "print(f\"- Categorical Features ({len(results['categorical_features'])}): {results['categorical_features']}\")\n",
        "print(f\"- Numerical Features ({len(results['numerical_features'])}): {results['numerical_features']}\\n\")\n",
        "\n",
        "print(\"🧠 RECOMMENDED ASSOCIATION RULE TYPES\")\n",
        "for rule, explanation in zip(results['recommended_rule_types']['types'], results['recommended_rule_types']['explanations']):\n",
        "    print(f\"✅ {rule}: {explanation}\")\n",
        "\n",
        "if results['boolean_like_columns']:\n",
        "    print(f\"\\n📌 Boolean-like Columns: {results['boolean_like_columns']}\")\n",
        "\n",
        "if results['negative_rule_candidates']:\n",
        "    print(\"\\n⚠️ Potential Negative Associations Found Between:\")\n",
        "    for pair in results['negative_rule_candidates']:\n",
        "        print(f\"   - {pair[0]} ⛔ {pair[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWEQAicg4nDN",
        "outputId": "0bd37585-76af-48a7-f477-eda252ad6c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 DATASET SUMMARY\n",
            "- Rows: 522064\n",
            "- Columns: 7\n",
            "- Categorical Features (4): ['BillNo', 'Itemname', 'CustomerID', 'Country']\n",
            "- Numerical Features (2): ['Quantity', 'Price']\n",
            "\n",
            "🧠 RECOMMENDED ASSOCIATION RULE TYPES\n",
            "✅ Multi-dimensional Association Rules: Multiple categorical columns found. Enables rules across dimensions (e.g., 'If Country is France and Customer is X, then likely buys Y').\n",
            "✅ Quantitative Association Rules: Numerical features detected. Enables mining based on numerical ranges (e.g., 'If Quantity > 10, then Price > 20').\n",
            "✅ Potential Negative Association Rules: Certain column pairs never or rarely occur together. May suggest exclusivity or negative relationships.\n",
            "\n",
            "⚠️ Potential Negative Associations Found Between:\n",
            "   - BillNo ⛔ Itemname\n",
            "   - BillNo ⛔ CustomerID\n",
            "   - BillNo ⛔ Country\n",
            "   - Itemname ⛔ CustomerID\n",
            "   - Itemname ⛔ Country\n",
            "   - CustomerID ⛔ Country\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend matplotlib seaborn"
      ],
      "metadata": {
        "id": "kCY3BmH35B4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "init(autoreset=True)\n",
        "\n",
        "# ------------------------ Sample Transactions\n",
        "transactions = [\n",
        "    ['milk', 'bread', 'eggs'],\n",
        "    ['milk', 'bread'],\n",
        "    ['milk', 'cookies'],\n",
        "    ['bread', 'butter'],\n",
        "    ['milk', 'bread', 'butter', 'eggs'],\n",
        "    ['cookies', 'soda'],\n",
        "    ['milk', 'soda'],\n",
        "]\n",
        "\n",
        "# ------------------------ Encode Transactions\n",
        "te = TransactionEncoder()\n",
        "df = pd.DataFrame(te.fit_transform(transactions), columns=te.columns_)\n",
        "\n",
        "# ------------------------ Grid Search Parameters\n",
        "support_range = [0.2, 0.3, 0.4]\n",
        "confidence_range = [0.5, 0.6, 0.7]\n",
        "grid_results = []\n",
        "\n",
        "# ------------------------ Apriori & FP-Growth Grid Search\n",
        "for algo_name, algo_func in [('Apriori', apriori), ('FP-Growth', fpgrowth)]:\n",
        "    for support, confidence in product(support_range, confidence_range):\n",
        "        start = time.time()\n",
        "        frequent_items = algo_func(df, min_support=support, use_colnames=True)\n",
        "        rules = association_rules(frequent_items, metric=\"confidence\", min_threshold=confidence)\n",
        "        elapsed = time.time() - start\n",
        "        if not rules.empty:\n",
        "            avg_lift = rules['lift'].mean()\n",
        "            avg_conf = rules['confidence'].mean()\n",
        "        else:\n",
        "            avg_lift = avg_conf = 0\n",
        "        score = len(rules) * avg_lift * avg_conf\n",
        "        grid_results.append({\n",
        "            'Algorithm': algo_name,\n",
        "            'Min Support': support,\n",
        "            'Min Confidence': confidence,\n",
        "            'Rules': len(rules),\n",
        "            'Avg Lift': round(avg_lift, 3),\n",
        "            'Avg Confidence': round(avg_conf, 3),\n",
        "            'Time (s)': round(elapsed, 4),\n",
        "            'Score': round(score, 3)\n",
        "        })\n",
        "\n",
        "# ------------------------ Summary DataFrame\n",
        "grid_df = pd.DataFrame(grid_results)\n",
        "print(Fore.CYAN + \"\\n📊 Top 5 Rule-Generating Configurations:\")\n",
        "print(grid_df.sort_values('Score', ascending=False).head(5))\n",
        "\n",
        "# ------------------------ Select Best Params\n",
        "best_params = grid_df.sort_values('Score', ascending=False).iloc[0]\n",
        "best_support = best_params['Min Support']\n",
        "best_conf = best_params['Min Confidence']\n",
        "\n",
        "print(Fore.GREEN + \"\\n✅ Best Parameter Combination Selected:\")\n",
        "print(f\"🔹 Algorithm: {best_params['Algorithm']}\")\n",
        "print(f\"🔹 Min Support: {best_support}\")\n",
        "print(f\"🔹 Min Confidence: {best_conf}\")\n",
        "print(f\"🔹 Rules Found: {int(best_params['Rules'])}\")\n",
        "print(f\"🔹 Avg Lift: {round(best_params['Avg Lift'], 3)}\")\n",
        "print(f\"🔹 Avg Confidence: {round(best_params['Avg Confidence'], 3)}\")\n",
        "\n",
        "# ------------------------ Generate Rules\n",
        "rules_apriori = association_rules(apriori(df, min_support=best_support, use_colnames=True),\n",
        "                                  metric=\"confidence\", min_threshold=best_conf)\n",
        "rules_apriori['Algorithm'] = 'Apriori'\n",
        "\n",
        "rules_fp = association_rules(fpgrowth(df, min_support=best_support, use_colnames=True),\n",
        "                             metric=\"confidence\", min_threshold=best_conf)\n",
        "rules_fp['Algorithm'] = 'FP-Growth'\n",
        "\n",
        "# ------------------------ ECLAT\n",
        "eclat_model = ECLAT(data=pd.DataFrame({'Transactions': transactions}), verbose=False)\n",
        "rule_eclat = eclat_model.fit(min_support=best_support)\n",
        "rules_eclat = pd.DataFrame(rule_eclat['rule_support'].items(), columns=['itemsets', 'support'])\n",
        "rules_eclat['itemsets'] = rules_eclat['itemsets'].apply(lambda x: frozenset(x))\n",
        "rules_eclat = rules_eclat[rules_eclat['itemsets'].apply(lambda x: len(x) >= 2)]\n",
        "rules_eclat['confidence'] = None\n",
        "rules_eclat['lift'] = None\n",
        "rules_eclat['antecedents'] = rules_eclat['itemsets'].apply(lambda x: frozenset(list(x)[:-1]))\n",
        "rules_eclat['consequents'] = rules_eclat['itemsets'].apply(lambda x: frozenset([list(x)[-1]]))\n",
        "rules_eclat['Algorithm'] = 'Eclat'\n",
        "rules_eclat = rules_eclat[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'Algorithm']]\n",
        "\n",
        "# ------------------------ Combine Rules\n",
        "combined_rules = pd.concat([rules_apriori, rules_fp, rules_eclat], ignore_index=True)\n",
        "\n",
        "# ------------------------ Low-Quality Rule Warning\n",
        "low_quality = combined_rules[\n",
        "    (combined_rules['lift'].fillna(0) < 1.0) |\n",
        "    (combined_rules['confidence'].fillna(0) < 0.5)\n",
        "]\n",
        "if len(low_quality) > 0:\n",
        "    print(Fore.YELLOW + f\"\\n⚠️ {len(low_quality)} low-quality rules found (lift < 1.0 or confidence < 0.5)\")\n",
        "else:\n",
        "    print(Fore.GREEN + f\"\\n✅ All {len(combined_rules)} rules are strong.\")\n",
        "\n",
        "# ------------------------ Show Top 5 Strongest Rules\n",
        "print(Fore.MAGENTA + \"\\n🔍 Top 5 High-Lift Rules:\")\n",
        "top_rules = combined_rules.dropna().sort_values(by='lift', ascending=False).head(5)\n",
        "for _, row in top_rules.iterrows():\n",
        "    ant, con = set(row['antecedents']), set(row['consequents'])\n",
        "    print(f\"• If {ant} → then {con} \"\n",
        "          f\"(support={round(row['support'], 2)}, conf={round(row['confidence'], 2)}, lift={round(row['lift'], 2)})\")\n",
        "\n",
        "# ------------------------ Export CSVs\n",
        "combined_rules.to_csv(\"association_rules.csv\", index=False)\n",
        "grid_df.to_csv(\"grid_search_results.csv\", index=False)\n",
        "print(Fore.CYAN + \"\\n📁 Exported: 'association_rules.csv' and 'grid_search_results.csv'\")\n",
        "\n",
        "# ------------------------ Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    data=combined_rules.dropna(subset=['support', 'confidence']),\n",
        "    x='support',\n",
        "    y='confidence',\n",
        "    hue='Algorithm',\n",
        "    size='lift',\n",
        "    sizes=(20, 200),\n",
        "    palette='Set2'\n",
        ")\n",
        "plt.title('📊 Association Rules: Support vs Confidence')\n",
        "plt.xlabel('Support')\n",
        "plt.ylabel('Confidence')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------------ Item-Based Collaborative Filtering (Cosine Similarity)\n",
        "def item_based_recommendation(item, user_item_matrix, cosine_sim, top_k=5):\n",
        "    item_idx = user_item_matrix.columns.get_loc(item)\n",
        "    item_similarities = cosine_sim[item_idx]\n",
        "\n",
        "    # Get the indices of the top_k most similar items\n",
        "    similar_items_idx = np.argsort(item_similarities)[::-1][1:top_k+1]\n",
        "    similar_items = user_item_matrix.columns[similar_items_idx]\n",
        "\n",
        "    print(Fore.BLUE + f\"\\n🎯 Recommendations for item: {item}\")\n",
        "    for i, similar_item in enumerate(similar_items):\n",
        "        print(f\"→ {similar_item} (cosine similarity={round(item_similarities[similar_items_idx[i]], 2)})\")\n",
        "\n",
        "# ------------------------ Prepare Data for Collaborative Filtering\n",
        "user_item_matrix = pd.DataFrame(te.transform(transactions).toarray(), columns=te.columns_)\n",
        "cosine_sim = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "# ------------------------ Popularity-Based Recommendation\n",
        "def popularity_based_recommendation(user_item_matrix, top_k=5):\n",
        "    item_popularity = user_item_matrix.sum(axis=0)\n",
        "    popular_items = item_popularity.sort_values(ascending=False).head(top_k)\n",
        "\n",
        "    print(Fore.GREEN + \"\\n🎯 Popularity-Based Recommendations:\")\n",
        "    for item, count in popular_items.items():\n",
        "        print(f\"→ {item} (purchased {count} times)\")\n",
        "\n",
        "# ------------------------ Hybrid Recommendation (Item-Based CF + Popularity)\n",
        "def hybrid_recommendation(item, user_item_matrix, cosine_sim, top_k=5, alpha=0.7):\n",
        "    # Get recommendations using item-based CF\n",
        "    item_similarities = cosine_sim[user_item_matrix.columns.get_loc(item)]\n",
        "    similar_items_idx = np.argsort(item_similarities)[::-1][1:top_k+1]\n",
        "    similar_items = user_item_matrix.columns[similar_items_idx]\n",
        "\n",
        "    # Get popularity-based recommendations\n",
        "    item_popularity = user_item_matrix.sum(axis=0)\n",
        "    popular_items = item_popularity.sort_values(ascending=False).head(top_k)\n",
        "\n",
        "    # Combine recommendations\n",
        "    print(Fore.MAGENTA + f\"\\n🎯 Hybrid Recommendations for item: {item}\")\n",
        "    print(Fore.YELLOW + f\"\\nItem-Based CF Recommendations:\")\n",
        "    for similar_item in similar_items:\n",
        "        print(f\"→ {similar_item} (cosine similarity={round(item_similarities[similar_items_idx[0]], 2)})\")\n",
        "\n",
        "    print(Fore.GREEN + f\"\\nPopularity-Based Recommendations:\")\n",
        "    for popular_item, count in popular_items.items():\n",
        "        print(f\"→ {popular_item} (purchased {count} times)\")\n",
        "\n",
        "# ------------------------ Example Usage\n",
        "item_based_recommendation('milk', user_item_matrix, cosine_sim)\n",
        "popularity_based_recommendation(user_item_matrix)\n",
        "hybrid_recommendation('milk', user_item_matrix, cosine_sim)"
      ],
      "metadata": {
        "id": "4nLS1YM8n-eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "\n",
        "init(autoreset=True)\n",
        "\n",
        "# ------------------------ Sample Transactions\n",
        "transactions = [\n",
        "    ['milk', 'bread', 'eggs'],\n",
        "    ['milk', 'bread'],\n",
        "    ['milk', 'cookies'],\n",
        "    ['bread', 'butter'],\n",
        "    ['milk', 'bread', 'butter', 'eggs'],\n",
        "    ['cookies', 'soda'],\n",
        "    ['milk', 'soda'],\n",
        "]\n",
        "\n",
        "# ------------------------ Encode Transactions\n",
        "te = TransactionEncoder()\n",
        "df = pd.DataFrame(te.fit_transform(transactions), columns=te.columns_)\n",
        "\n",
        "# ------------------------ Apriori, FP-Growth, ECLAT: Generate Association Rules\n",
        "support_range = [0.2, 0.3, 0.4]\n",
        "confidence_range = [0.5, 0.6, 0.7]\n",
        "rules_apriori = []\n",
        "rules_fp = []\n",
        "rules_eclat = []\n",
        "\n",
        "# Apriori and FP-Growth Rule Generation\n",
        "for support, confidence in product(support_range, confidence_range):\n",
        "    # Apriori\n",
        "    frequent_items_apriori = apriori(df, min_support=support, use_colnames=True)\n",
        "    rules_apriori.extend(association_rules(frequent_items_apriori, metric=\"confidence\", min_threshold=confidence).to_dict('records'))\n",
        "\n",
        "    # FP-Growth\n",
        "    frequent_items_fp = fpgrowth(df, min_support=support, use_colnames=True)\n",
        "    rules_fp.extend(association_rules(frequent_items_fp, metric=\"confidence\", min_threshold=confidence).to_dict('records'))\n",
        "\n",
        "# Eclat Rule Generation\n",
        "eclat_model = ECLAT(data=pd.DataFrame({'Transactions': transactions}), verbose=False)\n",
        "rule_eclat = eclat_model.fit(min_support=0.3)\n",
        "rules_eclat = pd.DataFrame(rule_eclat['rule_support'].items(), columns=['itemsets', 'support'])\n",
        "rules_eclat['itemsets'] = rules_eclat['itemsets'].apply(lambda x: frozenset(x))\n",
        "rules_eclat = rules_eclat[rules_eclat['itemsets'].apply(lambda x: len(x) >= 2)]\n",
        "rules_eclat['confidence'] = None\n",
        "rules_eclat['lift'] = None\n",
        "rules_eclat['antecedents'] = rules_eclat['itemsets'].apply(lambda x: frozenset(list(x)[:-1]))\n",
        "rules_eclat['consequents'] = rules_eclat['itemsets'].apply(lambda x: frozenset([list(x)[-1]]))\n",
        "\n",
        "# ------------------------ Generate Association Rules DataFrame\n",
        "rules_apriori_df = pd.DataFrame(rules_apriori)\n",
        "rules_fp_df = pd.DataFrame(rules_fp)\n",
        "rules_eclat_df = rules_eclat[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
        "\n",
        "# Combine all rules into one DataFrame\n",
        "combined_rules = pd.concat([rules_apriori_df, rules_fp_df, rules_eclat_df], ignore_index=True)\n",
        "\n",
        "# ------------------------ Prepare Data for Collaborative Filtering\n",
        "user_item_matrix = pd.DataFrame(te.transform(transactions).toarray(), columns=te.columns_)\n",
        "cosine_sim = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "# ------------------------ Item-Based Collaborative Filtering\n",
        "def item_based_recommendation(item, user_item_matrix, cosine_sim, top_k=5):\n",
        "    item_idx = user_item_matrix.columns.get_loc(item)\n",
        "    item_similarities = cosine_sim[item_idx]\n",
        "\n",
        "    # Get the indices of the top_k most similar items\n",
        "    similar_items_idx = np.argsort(item_similarities)[::-1][1:top_k+1]\n",
        "    similar_items = user_item_matrix.columns[similar_items_idx]\n",
        "\n",
        "    print(Fore.BLUE + f\"\\n🎯 Item-Based Recommendations for: {item}\")\n",
        "    for i, similar_item in enumerate(similar_items):\n",
        "        print(f\"→ {similar_item} (cosine similarity={round(item_similarities[similar_items_idx[i]], 2)})\")\n",
        "\n",
        "# ------------------------ Popularity-Based Recommendation\n",
        "def popularity_based_recommendation(user_item_matrix, top_k=5):\n",
        "    item_popularity = user_item_matrix.sum(axis=0)\n",
        "    popular_items = item_popularity.sort_values(ascending=False).head(top_k)\n",
        "\n",
        "    print(Fore.GREEN + \"\\n🎯 Popularity-Based Recommendations:\")\n",
        "    for item, count in popular_items.items():\n",
        "        print(f\"→ {item} (purchased {count} times)\")\n",
        "\n",
        "# ------------------------ Association Rule Recommendation\n",
        "def association_based_recommendation(item, combined_rules, top_k=5):\n",
        "    print(Fore.GREEN + f\"\\n🎯 Association-Based Recommendations for item: {item}\")\n",
        "\n",
        "    # Find rules where item is in the antecedent (left-hand side)\n",
        "    item_rules = combined_rules[combined_rules['antecedents'].apply(lambda x: item in x)]\n",
        "\n",
        "    # Sort rules by lift (stronger relationships have higher lift)\n",
        "    item_rules = item_rules.sort_values(by='lift', ascending=False)\n",
        "\n",
        "    # Recommend the top_k items based on the strongest rules\n",
        "    recommended_items = set()\n",
        "    for _, row in item_rules.head(top_k).iterrows():\n",
        "        recommended_items.update(row['consequents'])\n",
        "\n",
        "    # Remove the original item from recommendations\n",
        "    recommended_items.discard(item)\n",
        "\n",
        "    # Output recommendations\n",
        "    for rec_item in recommended_items:\n",
        "        print(f\"→ {rec_item} (Lift={round(row['lift'], 2)}, Confidence={round(row['confidence'], 2)})\")\n",
        "\n",
        "# ------------------------ Hybrid Recommendation (Combination of CF + Association Rules)\n",
        "def hybrid_recommendation(item, user_item_matrix, cosine_sim, combined_rules, top_k=5):\n",
        "    print(Fore.MAGENTA + f\"\\n🎯 Hybrid Recommendations for: {item}\")\n",
        "\n",
        "    # Get recommendations using item-based CF\n",
        "    item_similarities = cosine_sim[user_item_matrix.columns.get_loc(item)]\n",
        "    similar_items_idx = np.argsort(item_similarities)[::-1][1:top_k+1]\n",
        "    similar_items = user_item_matrix.columns[similar_items_idx]\n",
        "\n",
        "    # Get association rule-based recommendations\n",
        "    association_based_recommendation(item, combined_rules, top_k)\n",
        "\n",
        "    print(Fore.YELLOW + f\"\\nItem-Based CF Recommendations:\")\n",
        "    for similar_item in similar_items:\n",
        "        print(f\"→ {similar_item} (cosine similarity={round(item_similarities[similar_items_idx[0]], 2)})\")\n",
        "\n",
        "# ------------------------ Example Usage\n",
        "item_based_recommendation('milk', user_item_matrix, cosine_sim)\n",
        "popularity_based_recommendation(user_item_matrix)\n",
        "association_based_recommendation('milk', combined_rules, top_k=5)\n",
        "hybrid_recommendation('milk', user_item_matrix, cosine_sim, combined_rules, top_k=5)"
      ],
      "metadata": {
        "id": "KP7ffnuyoyRj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}