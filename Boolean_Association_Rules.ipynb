{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS2IpGaXneADvmnOeh0R1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeyScientist/Artificial-Intelligence/blob/main/Boolean_Association_Rules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "rYHxo1SCuZAm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "305985ed-3e4f-4e8e-d54e-89168121887d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0ec7530-8940-4ed7-8074-bd6a5873fb10\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d0ec7530-8940-4ed7-8074-bd6a5873fb10\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving single.csv to single.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyECLAT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIpOZTiIqJat",
        "outputId": "1c0bc34f-cfd4-4a78-b8e9-59377381f402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyECLAT\n",
            "  Downloading pyECLAT-1.0.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.11/dist-packages (from pyECLAT) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.4 in /usr/local/lib/python3.11/dist-packages (from pyECLAT) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from pyECLAT) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->pyECLAT) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->pyECLAT) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->pyECLAT) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->pyECLAT) (1.17.0)\n",
            "Downloading pyECLAT-1.0.2-py3-none-any.whl (6.3 kB)\n",
            "Installing collected packages: pyECLAT\n",
            "Successfully installed pyECLAT-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuYxVbMtqjsQ",
        "outputId": "3929f824-8244-43d3-d2a1-3645818d2c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the data (assuming semicolon-separated values)\n",
        "df = pd.read_csv('single.csv', sep=';')\n",
        "\n",
        "# Step 2: Clean column names to handle spaces or special characters\n",
        "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
        "df.columns = df.columns.str.replace(r'[^a-zA-Z0-9_]', '', regex=True)  # Remove special characters\n",
        "\n",
        "# Step 3: Print the cleaned column names to check if 'Country' exists\n",
        "print(\"Cleaned Column Names:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Step 4: Handle \"wrongly coded\" entries in 'Itemname'\n",
        "df['Itemname'] = df['Itemname'].replace(r'wrongly coded.*', 'Unknown Item', regex=True)\n",
        "\n",
        "# Step 5: Drop columns with completely empty values\n",
        "df = df.dropna(axis=1, how='all')\n",
        "\n",
        "# Step 6: Remove columns that start with 'Unnamed' (e.g., 'Country,,,')\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# Step 7: Clean 'Price' column: Replace commas with dots and convert to numeric\n",
        "df['Price'] = df['Price'].replace(',', '.', regex=True)  # Replace commas with dots\n",
        "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')  # Convert to numeric, coerce errors\n",
        "\n",
        "# Step 8: Clean 'Quantity' column: Convert to numeric\n",
        "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
        "\n",
        "# Step 9: Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Step 10: Handle missing values in 'CustomerID' and 'Country'\n",
        "# Ensure 'CustomerID' and 'Country' are available in cleaned columns\n",
        "if 'CustomerID' in df.columns:\n",
        "    df['CustomerID'] = df['CustomerID'].fillna('Unknown')  # Fill missing CustomerID with 'Unknown'\n",
        "else:\n",
        "    print(\"'CustomerID' column is missing!\")\n",
        "\n",
        "if 'Country' in df.columns:\n",
        "    df['Country'] = df['Country'].fillna('Unknown')  # Fill missing Country with 'Unknown'\n",
        "else:\n",
        "    print(\"'Country' column is missing!\")\n",
        "\n",
        "# Step 11: Show a snippet of the cleaned dataframe\n",
        "print(\"Cleaned DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 12: Check for any remaining \"Unknown Item\" entries (i.e., wrongly coded items)\n",
        "print(\"\\nRows with 'Unknown Item':\")\n",
        "print(df[df['Itemname'].str.contains('Unknown Item', na=False)])\n",
        "\n",
        "# Optional: Save the cleaned DataFrame to a new CSV file\n",
        "df.to_csv('cleaned_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr0OD_0Y0qn_",
        "outputId": "ffcd403c-3dba-436b-bd73-c414ea568b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Column Names:\n",
            "Index(['BillNo', 'Itemname', 'Quantity', 'Date', 'Price', 'CustomerID',\n",
            "       'Country'],\n",
            "      dtype='object')\n",
            "Cleaned DataFrame:\n",
            "   BillNo                             Itemname  Quantity                Date  \\\n",
            "0  536365   WHITE HANGING HEART T-LIGHT HOLDER       6.0 2010-01-12 08:26:00   \n",
            "1  536365                  WHITE METAL LANTERN       6.0 2010-01-12 08:26:00   \n",
            "2  536365       CREAM CUPID HEARTS COAT HANGER       8.0 2010-01-12 08:26:00   \n",
            "3  536365  KNITTED UNION FLAG HOT WATER BOTTLE       6.0 2010-01-12 08:26:00   \n",
            "4  536365       RED WOOLLY HOTTIE WHITE HEART.       6.0 2010-01-12 08:26:00   \n",
            "\n",
            "   Price CustomerID           Country  \n",
            "0   2.55    17850.0  United Kingdom,,  \n",
            "1   3.39    17850.0  United Kingdom,,  \n",
            "2   2.75    17850.0  United Kingdom,,  \n",
            "3   3.39    17850.0  United Kingdom,,  \n",
            "4   3.39    17850.0  United Kingdom,,  \n",
            "\n",
            "Rows with 'Unknown Item':\n",
            "        BillNo      Itemname  Quantity                Date  Price CustomerID  \\\n",
            "366291  569830  Unknown Item     800.0 2011-06-10 12:38:00    0.0    Unknown   \n",
            "366292  569831  Unknown Item    -800.0 2011-06-10 12:38:00    0.0    Unknown   \n",
            "406214  573114  Unknown Item    1000.0                 NaT    0.0    Unknown   \n",
            "406215  573115  Unknown Item   -1000.0                 NaT    0.0    Unknown   \n",
            "\n",
            "                  Country  \n",
            "366291  United Kingdom,,,  \n",
            "366292  United Kingdom,,,  \n",
            "406214  United Kingdom,,,  \n",
            "406215  United Kingdom,,,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2ctLvObariX4",
        "outputId": "bb495c9f-b4b8-40b9-c02f-b4261cfa130b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   BillNo                             Itemname  Quantity                Date  \\\n",
              "0  536365   WHITE HANGING HEART T-LIGHT HOLDER       6.0 2010-01-12 08:26:00   \n",
              "1  536365                  WHITE METAL LANTERN       6.0 2010-01-12 08:26:00   \n",
              "2  536365       CREAM CUPID HEARTS COAT HANGER       8.0 2010-01-12 08:26:00   \n",
              "3  536365  KNITTED UNION FLAG HOT WATER BOTTLE       6.0 2010-01-12 08:26:00   \n",
              "4  536365       RED WOOLLY HOTTIE WHITE HEART.       6.0 2010-01-12 08:26:00   \n",
              "\n",
              "   Price CustomerID           Country  \n",
              "0   2.55    17850.0  United Kingdom,,  \n",
              "1   3.39    17850.0  United Kingdom,,  \n",
              "2   2.75    17850.0  United Kingdom,,  \n",
              "3   3.39    17850.0  United Kingdom,,  \n",
              "4   3.39    17850.0  United Kingdom,,  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-acb139d3-d660-444f-b018-18b94d06e96a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BillNo</th>\n",
              "      <th>Itemname</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>536365</td>\n",
              "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2010-01-12 08:26:00</td>\n",
              "      <td>2.55</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>536365</td>\n",
              "      <td>WHITE METAL LANTERN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2010-01-12 08:26:00</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>536365</td>\n",
              "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2010-01-12 08:26:00</td>\n",
              "      <td>2.75</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>536365</td>\n",
              "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2010-01-12 08:26:00</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom,,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>536365</td>\n",
              "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2010-01-12 08:26:00</td>\n",
              "      <td>3.39</td>\n",
              "      <td>17850.0</td>\n",
              "      <td>United Kingdom,,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acb139d3-d660-444f-b018-18b94d06e96a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-acb139d3-d660-444f-b018-18b94d06e96a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-acb139d3-d660-444f-b018-18b94d06e96a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fef14d4f-842b-4fe1-a7c9-c0c684b8e126\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fef14d4f-842b-4fe1-a7c9-c0c684b8e126')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fef14d4f-842b-4fe1-a7c9-c0c684b8e126 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Itemname\"]=df[\"Itemname\"]\n",
        "df[\"Itemname\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "j1mYBtRy5jo7",
        "outputId": "bfaa4bc8-340e-4a02-d3f6-fc8ef654f78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          WHITE HANGING HEART T-LIGHT HOLDER\n",
              "1                         WHITE METAL LANTERN\n",
              "2              CREAM CUPID HEARTS COAT HANGER\n",
              "3         KNITTED UNION FLAG HOT WATER BOTTLE\n",
              "4              RED WOOLLY HOTTIE WHITE HEART.\n",
              "                         ...                 \n",
              "522059            PACK OF 20 SPACEBOY NAPKINS\n",
              "522060            CHILDREN'S APRON DOLLY GIRL\n",
              "522061           CHILDRENS CUTLERY DOLLY GIRL\n",
              "522062        CHILDRENS CUTLERY CIRCUS PARADE\n",
              "522063           BAKING SET 9 PIECE RETROSPOT\n",
              "Name: Itemname, Length: 522064, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Itemname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WHITE METAL LANTERN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522059</th>\n",
              "      <td>PACK OF 20 SPACEBOY NAPKINS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522060</th>\n",
              "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522061</th>\n",
              "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522062</th>\n",
              "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522063</th>\n",
              "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>522064 rows √ó 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        \"\"\"Evaluate rules using selected metric\"\"\"\n",
        "        if rules.empty:\n",
        "            return 0\n",
        "        return rules[self.metric].mean()\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='apriori',\n",
        "                    support_range=np.arange(0.1, 0.5, 0.1),\n",
        "                    confidence_range=np.arange(0.5, 0.9, 0.1)):\n",
        "        \"\"\"Automated grid search for optimal support and confidence\"\"\"\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                      total=len(support_range)*len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "            except Exception as e:\n",
        "                print(Fore.RED + f\"Error with support {support} and confidence {confidence}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if results:\n",
        "            return pd.DataFrame(results).sort_values('score', ascending=False)\n",
        "        else:\n",
        "            print(Fore.YELLOW + f\"No valid rules found for {algorithm.upper()} with current support/confidence ranges.\")\n",
        "            return pd.DataFrame(columns=['support', 'confidence', 'score', 'num_rules'])\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    \"\"\"Prepare transaction data from DataFrame\"\"\"\n",
        "    try:\n",
        "        transactions = df[items_column].dropna().apply(\n",
        "            lambda x: [item.strip() for item in str(x).split(',')]\n",
        "        )\n",
        "        print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "        return transactions.tolist()\n",
        "    except KeyError:\n",
        "        available_cols = df.columns.tolist()\n",
        "        print(f\"Error: Column '{items_column}' not found. Available columns: {available_cols}\")\n",
        "        for possible_name in ['items', 'products', 'item_name', 'item', 'Itemname']:\n",
        "            if possible_name in df.columns:\n",
        "                print(f\"Using alternative column name: {possible_name}\")\n",
        "                transactions = df[possible_name].dropna().apply(\n",
        "                    lambda x: [item.strip() for item in str(x).split(',')]\n",
        "                )\n",
        "                return transactions.tolist()\n",
        "        raise ValueError(f\"No suitable transaction column found. Available columns: {available_cols}\")\n",
        "\n",
        "def item_based_recommendation(item, encoded_df, cosine_sim, top_n=5):\n",
        "    \"\"\"Generate recommendations based on item similarity\"\"\"\n",
        "    try:\n",
        "        item_index = list(encoded_df.columns).index(item)\n",
        "        sim_scores = list(enumerate(cosine_sim[item_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:top_n+1]  # Exclude self and get top_n\n",
        "        recommendations = [encoded_df.columns[i] for i, _ in sim_scores]\n",
        "        print(Fore.GREEN + f\"\\nüéØ Item-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    except ValueError:\n",
        "        print(Fore.RED + f\"\\n‚ö†Ô∏è Item '{item}' not found in dataset\")\n",
        "        return []\n",
        "\n",
        "def popularity_based_recommendation(encoded_df, top_n=5):\n",
        "    \"\"\"Generate recommendations based on overall popularity\"\"\"\n",
        "    item_counts = encoded_df.sum().sort_values(ascending=False)\n",
        "    popular_items = list(item_counts.head(top_n).index)\n",
        "    print(Fore.GREEN + f\"\\nüèÜ Popularity-based recommendations: {popular_items}\")\n",
        "    return popular_items\n",
        "\n",
        "def association_based_recommendation(item, rules, top_n=5):\n",
        "    \"\"\"Generate recommendations based on association rules\"\"\"\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item in x)]\n",
        "    if not item_rules.empty:\n",
        "        item_rules = item_rules.sort_values('lift', ascending=False)\n",
        "        recommendations = list(set().union(*item_rules.head(top_n)['consequents'].apply(list)))\n",
        "        print(Fore.GREEN + f\"\\nüîó Association-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(Fore.YELLOW + f\"\\n‚ÑπÔ∏è No association rules found for '{item}'\")\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(item, encoded_df, cosine_sim, rules, top_n=5):\n",
        "    \"\"\"Combine multiple recommendation approaches\"\"\"\n",
        "    item_based = item_based_recommendation(item, encoded_df, cosine_sim, top_n)\n",
        "    assoc_based = association_based_recommendation(item, rules, top_n)\n",
        "    popular = popularity_based_recommendation(encoded_df, top_n)\n",
        "\n",
        "    # Combine and deduplicate recommendations\n",
        "    combined = list(set(item_based + assoc_based + popular))\n",
        "    combined = [i for i in combined if i != item]  # Remove self if present\n",
        "\n",
        "    print(Fore.BLUE + f\"\\n‚ú® Hybrid recommendations for '{item}': {combined[:top_n]}\")\n",
        "    return combined[:top_n]\n",
        "\n",
        "def generate_all_rules(transactions, metric='lift'):\n",
        "    \"\"\"Generate rules using all algorithms with optimal parameters\"\"\"\n",
        "    miner = AssociationRuleMiner(metric=metric)\n",
        "\n",
        "    # Grid search for Apriori\n",
        "    apriori_results = miner.grid_search(transactions, algorithm='apriori')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best Apriori Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # Grid search for FP-Growth\n",
        "    fp_results = miner.grid_search(transactions, algorithm='fpgrowth')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best FP-Growth Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # Get best rules from both algorithms\n",
        "    best_rules = miner.best_rules.copy()\n",
        "\n",
        "    # Add ECLAT rules (simplified implementation)\n",
        "    try:\n",
        "        eclat = ECLAT(data=pd.DataFrame({'Transactions': {i: t for i, t in enumerate(transactions)}}))\n",
        "        _, eclat_rules = eclat.fit(min_support=miner.best_params.get('support', 0.1))\n",
        "\n",
        "        eclat_formatted = []\n",
        "        for itemset, support in eclat_rules.items():\n",
        "            if len(itemset) >= 2:\n",
        "                # Here we create simple rules by splitting the itemset\n",
        "                for i in range(1, len(itemset)):\n",
        "                    eclat_formatted.append({\n",
        "                        'antecedents': frozenset(itemset[:i]),\n",
        "                        'consequents': frozenset(itemset[i:]),\n",
        "                        'support': support,\n",
        "                        'confidence': None,\n",
        "                        'lift': None\n",
        "                    })\n",
        "\n",
        "        eclat_df = pd.DataFrame(eclat_formatted)\n",
        "        best_rules = pd.concat([best_rules, eclat_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"ECLAT Error: {str(e)}\")\n",
        "\n",
        "    return best_rules, apriori_results, fp_results\n",
        "\n",
        "# MAIN EXECUTION WITH YOUR DATAFRAME\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your existing dataframe here\n",
        "    # df = pd.read_csv('your_data.csv')  # Uncomment and modify as needed\n",
        "\n",
        "    # Ensure you have a DataFrame named df with 'Itemname' column\n",
        "    try:\n",
        "        df\n",
        "        if 'Itemname' not in df.columns:\n",
        "            raise KeyError\n",
        "    except (NameError, KeyError):\n",
        "        print(Fore.RED + \"‚ö†Ô∏è Please load your DataFrame with 'Itemname' column first\")\n",
        "        # Create example data if no DataFrame exists\n",
        "        data = {\n",
        "            'Itemname': [\n",
        "                'milk,bread,eggs',\n",
        "                'milk,bread',\n",
        "                'bread,eggs',\n",
        "                'milk,eggs',\n",
        "                'bread,butter',\n",
        "                'milk,bread,butter,eggs'\n",
        "            ]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        print(Fore.YELLOW + \"‚ö†Ô∏è Using example data instead\")\n",
        "\n",
        "    # Prepare data\n",
        "    transactions = load_and_prepare_data(df, items_column='Itemname')\n",
        "\n",
        "    # Generate rules with automatic parameter tuning\n",
        "    best_rules, apriori_results, fp_results = generate_all_rules(transactions, metric='lift')\n",
        "\n",
        "    # Prepare recommendation engine components\n",
        "    te = TransactionEncoder()\n",
        "    encoded_matrix = te.fit_transform(transactions)\n",
        "    encoded_df = pd.DataFrame(encoded_matrix, columns=te.columns_)\n",
        "    cosine_sim = cosine_similarity(encoded_df.T)\n",
        "\n",
        "    # Example recommendations\n",
        "    test_item = 'milk'  # Change this to test different items\n",
        "    item_based_recommendation(test_item, encoded_df, cosine_sim)\n",
        "    popularity_based_recommendation(encoded_df)\n",
        "    association_based_recommendation(test_item, best_rules)\n",
        "    hybrid_recommendation(test_item, encoded_df, cosine_sim, best_rules)\n",
        "\n",
        "    # Show grid search results\n",
        "    print(Fore.CYAN + \"\\nüìä Apriori Grid Search Results:\")\n",
        "    print(apriori_results.head())\n",
        "\n",
        "    print(Fore.CYAN + \"\\nüìä FP-Growth Grid Search Results:\")\n",
        "    print(fp_results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "9miZ4pJosizS",
        "outputId": "385ed212-5fb4-464e-b55c-71ee85e6aa6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully loaded 999 transactions\n",
            "\n",
            "üîç Performing Grid Search for APRIORI (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 93.24it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No valid rules found for APRIORI with current support/confidence ranges.\n",
            "\n",
            "üèÜ Best Apriori Parameters:\n",
            "{}\n",
            "\n",
            "üîç Performing Grid Search for FPGROWTH (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 26.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No valid rules found for FPGROWTH with current support/confidence ranges.\n",
            "\n",
            "üèÜ Best FP-Growth Parameters:\n",
            "{}\n",
            "ECLAT Error: unhashable type: 'list'\n",
            "\n",
            "‚ö†Ô∏è Item 'milk' not found in dataset\n",
            "\n",
            "üèÜ Popularity-based recommendations: ['WHITE HANGING HEART T-LIGHT HOLDER', 'HAND WARMER UNION JACK', 'HAND WARMER SCOTTY DOG DESIGN', 'JAM MAKING SET PRINTED', 'RED WOOLLY HOTTIE WHITE HEART.']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'antecedents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4124011c619a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mitem_based_recommendation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mpopularity_based_recommendation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0massociation_based_recommendation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0mhybrid_recommendation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4124011c619a>\u001b[0m in \u001b[0;36massociation_based_recommendation\u001b[0;34m(item, rules, top_n)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massociation_based_recommendation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\"Generate recommendations based on association rules\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mitem_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'antecedents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lift'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'antecedents'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        \"\"\"Evaluate rules using selected metric\"\"\"\n",
        "        if rules.empty:\n",
        "            return 0\n",
        "        return rules[self.metric].mean()\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='apriori',\n",
        "                   support_range=np.arange(0.1, 0.5, 0.1),\n",
        "                   confidence_range=np.arange(0.5, 0.9, 0.1)):\n",
        "        \"\"\"Automated grid search for optimal support and confidence\"\"\"\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                      total=len(support_range)*len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "            except Exception as e:\n",
        "                print(Fore.YELLOW + f\"Warning: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Create DataFrame only if we have results\n",
        "        if results:\n",
        "            results_df = pd.DataFrame(results)\n",
        "            # Only sort if 'score' column exists\n",
        "            if 'score' in results_df.columns:\n",
        "                return results_df.sort_values('score', ascending=False)\n",
        "            return results_df\n",
        "        return pd.DataFrame()  # Return empty DataFrame if no results\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    \"\"\"Prepare transaction data from DataFrame\"\"\"\n",
        "    try:\n",
        "        transactions = df[items_column].dropna().apply(\n",
        "            lambda x: [item.strip() for item in str(x).split(',')]\n",
        "        )\n",
        "        print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "        return transactions.tolist()\n",
        "    except KeyError:\n",
        "        available_cols = df.columns.tolist()\n",
        "        print(f\"Error: Column '{items_column}' not found. Available columns: {available_cols}\")\n",
        "        for possible_name in ['items', 'products', 'item_name', 'item', 'Itemname']:\n",
        "            if possible_name in df.columns:\n",
        "                print(f\"Using alternative column name: {possible_name}\")\n",
        "                transactions = df[possible_name].dropna().apply(\n",
        "                    lambda x: [item.strip() for item in str(x).split(',')]\n",
        "                )\n",
        "                return transactions.tolist()\n",
        "        raise ValueError(f\"No suitable transaction column found. Available columns: {available_cols}\")\n",
        "\n",
        "def item_based_recommendation(item, encoded_df, cosine_sim, top_n=5):\n",
        "    \"\"\"Generate recommendations based on item similarity\"\"\"\n",
        "    try:\n",
        "        item_index = list(encoded_df.columns).index(item)\n",
        "        sim_scores = list(enumerate(cosine_sim[item_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:top_n+1]  # Exclude self and get top_n\n",
        "        recommendations = [encoded_df.columns[i] for i, _ in sim_scores]\n",
        "        print(Fore.GREEN + f\"\\nüéØ Item-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    except ValueError:\n",
        "        print(Fore.RED + f\"\\n‚ö†Ô∏è Item '{item}' not found in dataset\")\n",
        "        return []\n",
        "\n",
        "def popularity_based_recommendation(encoded_df, top_n=5):\n",
        "    \"\"\"Generate recommendations based on overall popularity\"\"\"\n",
        "    item_counts = encoded_df.sum().sort_values(ascending=False)\n",
        "    popular_items = list(item_counts.head(top_n).index)\n",
        "    print(Fore.GREEN + f\"\\nüèÜ Popularity-based recommendations: {popular_items}\")\n",
        "    return popular_items\n",
        "\n",
        "def association_based_recommendation(item, rules, top_n=5):\n",
        "    \"\"\"Generate recommendations based on association rules\"\"\"\n",
        "    if rules.empty:\n",
        "        print(Fore.YELLOW + \"\\n‚ÑπÔ∏è No association rules available\")\n",
        "        return []\n",
        "\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item in x)]\n",
        "    if not item_rules.empty:\n",
        "        item_rules = item_rules.sort_values('lift', ascending=False)\n",
        "        recommendations = list(set().union(*item_rules.head(top_n)['consequents'].apply(list)))\n",
        "        print(Fore.GREEN + f\"\\nüîó Association-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(Fore.YELLOW + f\"\\n‚ÑπÔ∏è No association rules found for '{item}'\")\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(item, encoded_df, cosine_sim, rules, top_n=5):\n",
        "    \"\"\"Combine multiple recommendation approaches\"\"\"\n",
        "    item_based = item_based_recommendation(item, encoded_df, cosine_sim, top_n)\n",
        "    assoc_based = association_based_recommendation(item, rules, top_n)\n",
        "    popular = popularity_based_recommendation(encoded_df, top_n)\n",
        "\n",
        "    # Combine and deduplicate recommendations\n",
        "    combined = list(set(item_based + assoc_based + popular))\n",
        "    combined = [i for i in combined if i != item]  # Remove self if present\n",
        "\n",
        "    print(Fore.BLUE + f\"\\n‚ú® Hybrid recommendations for '{item}': {combined[:top_n]}\")\n",
        "    return combined[:top_n]\n",
        "\n",
        "def generate_all_rules(transactions, metric='lift'):\n",
        "    \"\"\"Generate rules using all algorithms with optimal parameters\"\"\"\n",
        "    miner = AssociationRuleMiner(metric=metric)\n",
        "\n",
        "    # Grid search for Apriori\n",
        "    apriori_results = miner.grid_search(transactions, algorithm='apriori')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best Apriori Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # Grid search for FP-Growth\n",
        "    fp_results = miner.grid_search(transactions, algorithm='fpgrowth')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best FP-Growth Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # Get best rules from both algorithms\n",
        "    best_rules = miner.best_rules\n",
        "\n",
        "    # Add ECLAT rules (simplified implementation)\n",
        "    try:\n",
        "        eclat = ECLAT(data=pd.DataFrame({'Transactions': {i: t for i, t in enumerate(transactions)}}))\n",
        "        _, eclat_rules = eclat.fit(min_support=miner.best_params.get('support', 0.1))\n",
        "\n",
        "        eclat_formatted = []\n",
        "        for itemset, support in eclat_rules.items():\n",
        "            if len(itemset) >= 2:\n",
        "                for i in range(1, len(itemset)):\n",
        "                    eclat_formatted.append({\n",
        "                        'antecedents': frozenset(itemset[:i]),\n",
        "                        'consequents': frozenset(itemset[i:]),\n",
        "                        'support': support,\n",
        "                        'confidence': None,\n",
        "                        'lift': None\n",
        "                    })\n",
        "\n",
        "        eclat_df = pd.DataFrame(eclat_formatted)\n",
        "        best_rules = pd.concat([best_rules, eclat_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"ECLAT Error: {str(e)}\")\n",
        "\n",
        "    return best_rules, apriori_results, fp_results\n",
        "\n",
        "# MAIN EXECUTION WITH YOUR DATAFRAME\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your existing dataframe here\n",
        "    # df = pd.read_csv('your_data.csv')  # Uncomment and modify as needed\n",
        "\n",
        "    # Ensure you have a DataFrame named df with 'Itemname' column\n",
        "    if 'df' not in locals() or 'Itemname' not in df.columns:\n",
        "        print(Fore.RED + \"‚ö†Ô∏è Please load your DataFrame with 'Itemname' column first\")\n",
        "        # Create example data if no DataFrame exists\n",
        "        data = {\n",
        "            'Itemname': [\n",
        "                'milk,bread,eggs',\n",
        "                'milk,bread',\n",
        "                'bread,eggs',\n",
        "                'milk,eggs',\n",
        "                'bread,butter',\n",
        "                'milk,bread,butter,eggs'\n",
        "            ]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        print(Fore.YELLOW + \"‚ö†Ô∏è Using example data instead\")\n",
        "\n",
        "    # Prepare data\n",
        "    transactions = load_and_prepare_data(df, items_column='Itemname')\n",
        "\n",
        "    # Generate rules with automatic parameter tuning\n",
        "    best_rules, apriori_results, fp_results = generate_all_rules(transactions, metric='lift')\n",
        "\n",
        "    # Only proceed if we got valid results\n",
        "    if not best_rules.empty:\n",
        "        # Prepare recommendation engine\n",
        "        te = TransactionEncoder()\n",
        "        encoded_matrix = te.fit_transform(transactions)\n",
        "        encoded_df = pd.DataFrame(encoded_matrix, columns=te.columns_)\n",
        "        cosine_sim = cosine_similarity(encoded_df.T)\n",
        "\n",
        "        # Example recommendations\n",
        "        test_item = 'milk'  # Change this to test different items\n",
        "        item_based_recommendation(test_item, encoded_df, cosine_sim)\n",
        "        popularity_based_recommendation(encoded_df)\n",
        "        association_based_recommendation(test_item, best_rules)\n",
        "        hybrid_recommendation(test_item, encoded_df, cosine_sim, best_rules)\n",
        "\n",
        "        # Show grid search results\n",
        "        if not apriori_results.empty:\n",
        "            print(Fore.CYAN + \"\\nüìä Apriori Grid Search Results:\")\n",
        "            print(apriori_results.head())\n",
        "\n",
        "        if not fp_results.empty:\n",
        "            print(Fore.CYAN + \"\\nüìä FP-Growth Grid Search Results:\")\n",
        "            print(fp_results.head())\n",
        "    else:\n",
        "        print(Fore.RED + \"‚ö†Ô∏è No valid rules were generated from the data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXu17zGKp4oR",
        "outputId": "4925990f-78f6-4f9b-d659-7847581de470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Please load your DataFrame with 'Itemname' column first\n",
            "‚ö†Ô∏è Using example data instead\n",
            "‚úÖ Successfully loaded 6 transactions\n",
            "\n",
            "üîç Performing Grid Search for APRIORI (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 26.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ Best Apriori Parameters:\n",
            "{'support': np.float64(0.1), 'confidence': np.float64(0.7999999999999999), 'algorithm': 'apriori'}\n",
            "\n",
            "üîç Performing Grid Search for FPGROWTH (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 32.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ Best FP-Growth Parameters:\n",
            "{'support': np.float64(0.1), 'confidence': np.float64(0.7999999999999999), 'algorithm': 'fpgrowth'}\n",
            "ECLAT Error: unhashable type: 'list'\n",
            "\n",
            "üéØ Item-based recommendations for 'milk': ['eggs', 'bread', 'butter']\n",
            "\n",
            "üèÜ Popularity-based recommendations: ['bread', 'eggs', 'milk', 'butter']\n",
            "\n",
            "üîó Association-based recommendations for 'milk': ['bread', 'eggs']\n",
            "\n",
            "üéØ Item-based recommendations for 'milk': ['eggs', 'bread', 'butter']\n",
            "\n",
            "üîó Association-based recommendations for 'milk': ['bread', 'eggs']\n",
            "\n",
            "üèÜ Popularity-based recommendations: ['bread', 'eggs', 'milk', 'butter']\n",
            "\n",
            "‚ú® Hybrid recommendations for 'milk': ['bread', 'eggs', 'butter']\n",
            "\n",
            "üìä Apriori Grid Search Results:\n",
            "    support  confidence     score  num_rules\n",
            "3       0.1         0.8  1.480000         10\n",
            "2       0.1         0.7  1.346429         14\n",
            "1       0.1         0.6  1.234211         19\n",
            "11      0.3         0.8  1.200000          1\n",
            "7       0.2         0.8  1.200000          1\n",
            "\n",
            "üìä FP-Growth Grid Search Results:\n",
            "    support  confidence     score  num_rules\n",
            "3       0.1         0.8  1.480000         10\n",
            "2       0.1         0.7  1.346429         14\n",
            "1       0.1         0.6  1.234211         19\n",
            "11      0.3         0.8  1.200000          1\n",
            "7       0.2         0.8  1.200000          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        \"\"\"Evaluate rules using selected metric\"\"\"\n",
        "        if rules.empty:\n",
        "            return 0\n",
        "        return rules[self.metric].mean()\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='apriori',\n",
        "                   support_range=np.arange(0.1, 0.5, 0.1),\n",
        "                   confidence_range=np.arange(0.5, 0.9, 0.1)):\n",
        "        \"\"\"Automated grid search for optimal support and confidence\"\"\"\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                      total=len(support_range)*len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "            except Exception as e:\n",
        "                print(Fore.YELLOW + f\"Warning: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Create DataFrame only if we have results\n",
        "        if results:\n",
        "            results_df = pd.DataFrame(results)\n",
        "            # Only sort if 'score' column exists\n",
        "            if 'score' in results_df.columns:\n",
        "                return results_df.sort_values('score', ascending=False)\n",
        "            return results_df\n",
        "        return pd.DataFrame()  # Return empty DataFrame if no results\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    \"\"\"Prepare transaction data from DataFrame\"\"\"\n",
        "    try:\n",
        "        transactions = df[items_column].dropna().apply(\n",
        "            lambda x: [item.strip() for item in str(x).split(',')]\n",
        "        )\n",
        "        print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "        return transactions.tolist()\n",
        "    except KeyError:\n",
        "        available_cols = df.columns.tolist()\n",
        "        print(f\"Error: Column '{items_column}' not found. Available columns: {available_cols}\")\n",
        "        for possible_name in ['items', 'products', 'item_name', 'item', 'Itemname']:\n",
        "            if possible_name in df.columns:\n",
        "                print(f\"Using alternative column name: {possible_name}\")\n",
        "                transactions = df[possible_name].dropna().apply(\n",
        "                    lambda x: [item.strip() for item in str(x).split(',')]\n",
        "                )\n",
        "                return transactions.tolist()\n",
        "        raise ValueError(f\"No suitable transaction column found. Available columns: {available_cols}\")\n",
        "\n",
        "def item_based_recommendation(item, encoded_df, cosine_sim, top_n=5):\n",
        "    \"\"\"Generate recommendations based on item similarity\"\"\"\n",
        "    try:\n",
        "        item_index = list(encoded_df.columns).index(item)\n",
        "        sim_scores = list(enumerate(cosine_sim[item_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:top_n+1]  # Exclude self and get top_n\n",
        "        recommendations = [encoded_df.columns[i] for i, _ in sim_scores]\n",
        "        print(Fore.GREEN + f\"\\nüéØ Item-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    except ValueError:\n",
        "        print(Fore.RED + f\"\\n‚ö†Ô∏è Item '{item}' not found in dataset\")\n",
        "        return []\n",
        "\n",
        "def popularity_based_recommendation(encoded_df, top_n=5):\n",
        "    \"\"\"Generate recommendations based on overall popularity\"\"\"\n",
        "    item_counts = encoded_df.sum().sort_values(ascending=False)\n",
        "    popular_items = list(item_counts.head(top_n).index)\n",
        "    print(Fore.GREEN + f\"\\nüèÜ Popularity-based recommendations: {popular_items}\")\n",
        "    return popular_items\n",
        "\n",
        "def association_based_recommendation(item, rules, top_n=5):\n",
        "    \"\"\"Generate recommendations based on association rules\"\"\"\n",
        "    if rules.empty:\n",
        "        print(Fore.YELLOW + \"\\n‚ÑπÔ∏è No association rules available\")\n",
        "        return []\n",
        "\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item in x)]\n",
        "    if not item_rules.empty:\n",
        "        item_rules = item_rules.sort_values('lift', ascending=False)\n",
        "        recommendations = list(set().union(*item_rules.head(top_n)['consequents'].apply(list)))\n",
        "        print(Fore.GREEN + f\"\\nüîó Association-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(Fore.YELLOW + f\"\\n‚ÑπÔ∏è No association rules found for '{item}'\")\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(item, encoded_df, cosine_sim, rules, top_n=5):\n",
        "    \"\"\"Combine multiple recommendation approaches\"\"\"\n",
        "    item_based = item_based_recommendation(item, encoded_df, cosine_sim, top_n)\n",
        "    assoc_based = association_based_recommendation(item, rules, top_n)\n",
        "    popular = popularity_based_recommendation(encoded_df, top_n)\n",
        "\n",
        "    # Combine and deduplicate recommendations\n",
        "    combined = list(set(item_based + assoc_based + popular))\n",
        "    combined = [i for i in combined if i != item]  # Remove self if present\n",
        "\n",
        "    print(Fore.BLUE + f\"\\n‚ú® Hybrid recommendations for '{item}': {combined[:top_n]}\")\n",
        "    return combined[:top_n]\n",
        "\n",
        "def generate_all_rules(transactions, metric='lift'):\n",
        "    \"\"\"Generate rules using all algorithms with optimal parameters\"\"\"\n",
        "    miner = AssociationRuleMiner(metric=metric)\n",
        "\n",
        "    # Grid search for Apriori\n",
        "    apriori_results = miner.grid_search(transactions, algorithm='apriori')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best Apriori Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # Grid search for FP-Growth\n",
        "    fp_results = miner.grid_search(transactions, algorithm='fpgrowth')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best FP-Growth Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # Get best rules from both algorithms\n",
        "    best_rules = miner.best_rules\n",
        "\n",
        "    # Add ECLAT rules (simplified implementation)\n",
        "    try:\n",
        "        eclat = ECLAT(data=pd.DataFrame({'Transactions': {i: t for i, t in enumerate(transactions)}}))\n",
        "        _, eclat_rules = eclat.fit(min_support=miner.best_params.get('support', 0.1))\n",
        "\n",
        "        eclat_formatted = []\n",
        "        for itemset, support in eclat_rules.items():\n",
        "            if len(itemset) >= 2:\n",
        "                for i in range(1, len(itemset)):\n",
        "                    eclat_formatted.append({\n",
        "                        'antecedents': frozenset(itemset[:i]),\n",
        "                        'consequents': frozenset(itemset[i:]),\n",
        "                        'support': support,\n",
        "                        'confidence': None,\n",
        "                        'lift': None\n",
        "                    })\n",
        "\n",
        "        eclat_df = pd.DataFrame(eclat_formatted)\n",
        "        best_rules = pd.concat([best_rules, eclat_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"ECLAT Error: {str(e)}\")\n",
        "\n",
        "    return best_rules, apriori_results, fp_results\n",
        "\n",
        "# MAIN EXECUTION WITH YOUR DATAFRAME\n",
        "\n",
        "\n",
        "\n",
        "    # Generate rules with automatic parameter tuning\n",
        "    best_rules, apriori_results, fp_results = generate_all_rules(df[\"itemname\"], metric='lift')\n",
        "\n",
        "    # Only proceed if we got valid results\n",
        "    if not best_rules.empty:\n",
        "        # Prepare recommendation engine\n",
        "        te = TransactionEncoder()\n",
        "        encoded_matrix = te.fit_transform(transactions)\n",
        "        encoded_df = pd.DataFrame(encoded_matrix, columns=te.columns_)\n",
        "        cosine_sim = cosine_similarity(encoded_df.T)\n",
        "\n",
        "        # Example recommendations\n",
        "        test_item = 'milk'  # Change this to test different items\n",
        "        item_based_recommendation(test_item, encoded_df, cosine_sim)\n",
        "        popularity_based_recommendation(encoded_df)\n",
        "        association_based_recommendation(test_item, best_rules)\n",
        "        hybrid_recommendation(test_item, encoded_df, cosine_sim, best_rules)\n",
        "\n",
        "        # Show grid search results\n",
        "        if not apriori_results.empty:\n",
        "            print(Fore.CYAN + \"\\nüìä Apriori Grid Search Results:\")\n",
        "            print(apriori_results.head())\n",
        "\n",
        "        if not fp_results.empty:\n",
        "            print(Fore.CYAN + \"\\nüìä FP-Growth Grid Search Results:\")\n",
        "            print(fp_results.head())\n",
        "    else:\n",
        "        print(Fore.RED + \"‚ö†Ô∏è No valid rules were generated from the data\")"
      ],
      "metadata": {
        "id": "_S1qscf67p7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"itemname\"]"
      ],
      "metadata": {
        "id": "l-qs3YiZqU9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "80878446-d6ba-471b-b013-b23579739e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'itemname'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'itemname'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2e3bce136b8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"itemname\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'itemname'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        \"\"\"Evaluate rules using selected metric\"\"\"\n",
        "        if rules.empty or self.metric not in rules.columns:\n",
        "            return 0\n",
        "        return rules[self.metric].mean()\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='apriori',\n",
        "                    support_range=np.arange(0.1, 0.5, 0.1),\n",
        "                    confidence_range=np.arange(0.5, 0.9, 0.1)):\n",
        "        \"\"\"Automated grid search for optimal support and confidence\"\"\"\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                        total=len(support_range) * len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "            except Exception as e:\n",
        "                print(Fore.RED + f\"Error with support {support} and confidence {confidence}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if results:\n",
        "            return pd.DataFrame(results).sort_values('score', ascending=False)\n",
        "        else:\n",
        "            print(Fore.YELLOW + f\"No valid rules found for {algorithm.upper()} with current support/confidence ranges.\")\n",
        "            return pd.DataFrame(columns=['support', 'confidence', 'score', 'num_rules'])\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    \"\"\"Prepare transaction data from DataFrame\"\"\"\n",
        "    transactions = df[items_column].dropna().apply(\n",
        "        lambda x: [item.strip() for item in str(x).split(',')]\n",
        "    )\n",
        "    print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "    return transactions.tolist()\n",
        "\n",
        "def item_based_recommendation(item, encoded_df, cosine_sim, top_n=5):\n",
        "    \"\"\"Generate recommendations based on item similarity\"\"\"\n",
        "    try:\n",
        "        item_index = list(encoded_df.columns).index(item)\n",
        "        sim_scores = list(enumerate(cosine_sim[item_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:top_n + 1]  # Exclude self\n",
        "        recommendations = [encoded_df.columns[i] for i, _ in sim_scores]\n",
        "        print(Fore.GREEN + f\"\\nüéØ Item-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    except ValueError:\n",
        "        print(Fore.RED + f\"\\n‚ö†Ô∏è Item '{item}' not found in dataset\")\n",
        "        return []\n",
        "\n",
        "def popularity_based_recommendation(encoded_df, top_n=5):\n",
        "    \"\"\"Generate recommendations based on overall popularity\"\"\"\n",
        "    item_counts = encoded_df.sum().sort_values(ascending=False)\n",
        "    popular_items = list(item_counts.head(top_n).index)\n",
        "    print(Fore.GREEN + f\"\\nüèÜ Popularity-based recommendations: {popular_items}\")\n",
        "    return popular_items\n",
        "\n",
        "def association_based_recommendation(item, rules, top_n=5):\n",
        "    \"\"\"Generate recommendations based on association rules\"\"\"\n",
        "    if 'antecedents' not in rules.columns or 'consequents' not in rules.columns:\n",
        "        print(Fore.YELLOW + \"‚ö†Ô∏è No valid association rules available.\")\n",
        "        return []\n",
        "\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item in x)]\n",
        "    if not item_rules.empty:\n",
        "        item_rules = item_rules.sort_values('lift', ascending=False)\n",
        "        recommendations = list(set().union(*item_rules.head(top_n)['consequents'].apply(list)))\n",
        "        print(Fore.GREEN + f\"\\nüîó Association-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(Fore.YELLOW + f\"\\n‚ÑπÔ∏è No association rules found for '{item}'\")\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(item, encoded_df, cosine_sim, rules, top_n=5):\n",
        "    \"\"\"Combine multiple recommendation approaches\"\"\"\n",
        "    item_based = item_based_recommendation(item, encoded_df, cosine_sim, top_n)\n",
        "    assoc_based = association_based_recommendation(item, rules, top_n)\n",
        "    popular = popularity_based_recommendation(encoded_df, top_n)\n",
        "\n",
        "    combined = list(set(item_based + assoc_based + popular))\n",
        "    combined = [i for i in combined if i != item]\n",
        "    print(Fore.BLUE + f\"\\n‚ú® Hybrid recommendations for '{item}': {combined[:top_n]}\")\n",
        "    return combined[:top_n]\n",
        "\n",
        "def generate_all_rules(transactions, metric='lift'):\n",
        "    \"\"\"Generate rules using all algorithms with optimal parameters\"\"\"\n",
        "    miner = AssociationRuleMiner(metric=metric)\n",
        "\n",
        "    # Apriori\n",
        "    apriori_results = miner.grid_search(transactions, algorithm='apriori')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best Apriori Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # FP-Growth\n",
        "    fp_results = miner.grid_search(transactions, algorithm='fpgrowth')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best FP-Growth Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # ECLAT\n",
        "    best_rules = miner.best_rules.copy()\n",
        "    try:\n",
        "        eclat = ECLAT(data=pd.DataFrame({'Transactions': {i: t for i, t in enumerate(transactions)}}))\n",
        "        _, eclat_rules = eclat.fit(min_support=miner.best_params.get('support', 0.1))\n",
        "\n",
        "        eclat_formatted = []\n",
        "        for itemset, support in eclat_rules.items():\n",
        "            if len(itemset) >= 2:\n",
        "                for i in range(1, len(itemset)):\n",
        "                    eclat_formatted.append({\n",
        "                        'antecedents': frozenset(itemset[:i]),\n",
        "                        'consequents': frozenset(itemset[i:]),\n",
        "                        'support': support,\n",
        "                        'confidence': None,\n",
        "                        'lift': None\n",
        "                    })\n",
        "\n",
        "        eclat_df = pd.DataFrame(eclat_formatted)\n",
        "        best_rules = pd.concat([best_rules, eclat_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"ECLAT Error: {str(e)}\")\n",
        "\n",
        "    return best_rules, apriori_results, fp_results\n",
        "\n",
        "# ===== MAIN EXECUTION =====\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure your DataFrame 'df' is loaded and has 'itemname'\n",
        "    if 'itemname' not in df.columns:\n",
        "        raise ValueError(\"‚ùå Column 'itemname' not found in your DataFrame.\")\n",
        "\n",
        "    df.rename(columns={'itemname': 'Itemname'}, inplace=True)\n",
        "    transactions = load_and_prepare_data(df, items_column='Itemname')\n",
        "\n",
        "    best_rules, apriori_results, fp_results = generate_all_rules(transactions, metric='lift')\n",
        "\n",
        "    te = TransactionEncoder()\n",
        "    encoded_matrix = te.fit_transform(transactions)\n",
        "    encoded_df = pd.DataFrame(encoded_matrix, columns=te.columns_)\n",
        "    cosine_sim = cosine_similarity(encoded_df.T)\n",
        "\n",
        "    test_item = 'milk'  # change this to any item in your dataset\n",
        "    item_based_recommendation(test_item, encoded_df, cosine_sim)\n",
        "    popularity_based_recommendation(encoded_df)\n",
        "    association_based_recommendation(test_item, best_rules)\n",
        "    hybrid_recommendation(test_item, encoded_df, cosine_sim, best_rules)\n",
        "\n",
        "    print(Fore.CYAN + \"\\nüìä Apriori Grid Search Results:\")\n",
        "    print(apriori_results.head())\n",
        "\n",
        "    print(Fore.CYAN + \"\\nüìä FP-Growth Grid Search Results:\")\n",
        "    print(fp_results.head())"
      ],
      "metadata": {
        "id": "xZf5j5ZRqWl3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "818118c3-818d-4864-898a-f319c304be33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "‚ùå Column 'itemname' not found in your DataFrame.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-db6f93fa0ad8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# Ensure your DataFrame 'df' is loaded and has 'itemname'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'itemname'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ùå Column 'itemname' not found in your DataFrame.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'itemname'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Itemname'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ‚ùå Column 'itemname' not found in your DataFrame."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        \"\"\"Evaluate rules using selected metric\"\"\"\n",
        "        if rules.empty or self.metric not in rules.columns:\n",
        "            return 0\n",
        "        return rules[self.metric].mean()\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='apriori',\n",
        "                    support_range=np.arange(0.1, 0.5, 0.1),\n",
        "                    confidence_range=np.arange(0.5, 0.9, 0.1)):\n",
        "        \"\"\"Automated grid search for optimal support and confidence\"\"\"\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                        total=len(support_range) * len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "            except Exception as e:\n",
        "                print(Fore.RED + f\"Error with support {support} and confidence {confidence}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if results:\n",
        "            return pd.DataFrame(results).sort_values('score', ascending=False)\n",
        "        else:\n",
        "            print(Fore.YELLOW + f\"No valid rules found for {algorithm.upper()} with current support/confidence ranges.\")\n",
        "            return pd.DataFrame(columns=['support', 'confidence', 'score', 'num_rules'])\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    \"\"\"Prepare transaction data from DataFrame\"\"\"\n",
        "    transactions = df[items_column].dropna().apply(\n",
        "        lambda x: [item.strip() for item in str(x).split(',')]\n",
        "    )\n",
        "    print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "    return transactions.tolist()\n",
        "\n",
        "def item_based_recommendation(item, encoded_df, cosine_sim, top_n=5):\n",
        "    \"\"\"Generate recommendations based on item similarity\"\"\"\n",
        "    try:\n",
        "        item_index = list(encoded_df.columns).index(item)\n",
        "        sim_scores = list(enumerate(cosine_sim[item_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:top_n + 1]  # Exclude self\n",
        "        recommendations = [encoded_df.columns[i] for i, _ in sim_scores]\n",
        "        print(Fore.GREEN + f\"\\nüéØ Item-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    except ValueError:\n",
        "        print(Fore.RED + f\"\\n‚ö†Ô∏è Item '{item}' not found in dataset\")\n",
        "        return []\n",
        "\n",
        "def popularity_based_recommendation(encoded_df, top_n=5):\n",
        "    \"\"\"Generate recommendations based on overall popularity\"\"\"\n",
        "    item_counts = encoded_df.sum().sort_values(ascending=False)\n",
        "    popular_items = list(item_counts.head(top_n).index)\n",
        "    print(Fore.GREEN + f\"\\nüèÜ Popularity-based recommendations: {popular_items}\")\n",
        "    return popular_items\n",
        "\n",
        "def association_based_recommendation(item, rules, top_n=5):\n",
        "    \"\"\"Generate recommendations based on association rules\"\"\"\n",
        "    if 'antecedents' not in rules.columns or 'consequents' not in rules.columns:\n",
        "        print(Fore.YELLOW + \"‚ö†Ô∏è No valid association rules available.\")\n",
        "        return []\n",
        "\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item in x)]\n",
        "    if not item_rules.empty:\n",
        "        item_rules = item_rules.sort_values('lift', ascending=False)\n",
        "        recommendations = list(set().union(*item_rules.head(top_n)['consequents'].apply(list)))\n",
        "        print(Fore.GREEN + f\"\\nüîó Association-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(Fore.YELLOW + f\"\\n‚ÑπÔ∏è No association rules found for '{item}'\")\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(item, encoded_df, cosine_sim, rules, top_n=5):\n",
        "    \"\"\"Combine multiple recommendation approaches\"\"\"\n",
        "    item_based = item_based_recommendation(item, encoded_df, cosine_sim, top_n)\n",
        "    assoc_based = association_based_recommendation(item, rules, top_n)\n",
        "    popular = popularity_based_recommendation(encoded_df, top_n)\n",
        "\n",
        "    combined = list(set(item_based + assoc_based + popular))\n",
        "    combined = [i for i in combined if i != item]\n",
        "    print(Fore.BLUE + f\"\\n‚ú® Hybrid recommendations for '{item}': {combined[:top_n]}\")\n",
        "    return combined[:top_n]\n",
        "\n",
        "def generate_all_rules(transactions, metric='lift'):\n",
        "    \"\"\"Generate rules using all algorithms with optimal parameters\"\"\"\n",
        "    miner = AssociationRuleMiner(metric=metric)\n",
        "\n",
        "    # Apriori\n",
        "    apriori_results = miner.grid_search(transactions, algorithm='apriori')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best Apriori Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # FP-Growth\n",
        "    fp_results = miner.grid_search(transactions, algorithm='fpgrowth')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best FP-Growth Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # ECLAT\n",
        "    best_rules = miner.best_rules.copy()\n",
        "    try:\n",
        "        eclat = ECLAT(data=pd.DataFrame({'Transactions': {i: t for i, t in enumerate(transactions)}}))\n",
        "        _, eclat_rules = eclat.fit(min_support=miner.best_params.get('support', 0.1))\n",
        "\n",
        "        eclat_formatted = []\n",
        "        for itemset, support in eclat_rules.items():\n",
        "            if len(itemset) >= 2:\n",
        "                for i in range(1, len(itemset)):\n",
        "                    eclat_formatted.append({\n",
        "                        'antecedents': frozenset(itemset[:i]),\n",
        "                        'consequents': frozenset(itemset[i:]),\n",
        "                        'support': support,\n",
        "                        'confidence': None,\n",
        "                        'lift': None\n",
        "                    })\n",
        "\n",
        "        eclat_df = pd.DataFrame(eclat_formatted)\n",
        "        best_rules = pd.concat([best_rules, eclat_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"ECLAT Error: {str(e)}\")\n",
        "\n",
        "    return best_rules, apriori_results, fp_results\n",
        "\n",
        "# ===== MAIN EXECUTION =====\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure your DataFrame 'df' is loaded and has 'Itemname' column\n",
        "    if 'Itemname' not in df.columns:\n",
        "        raise ValueError(\"‚ùå Column 'Itemname' not found in your DataFrame.\")\n",
        "\n",
        "    transactions = load_and_prepare_data(df, items_column='Itemname')\n",
        "\n",
        "    best_rules, apriori_results, fp_results = generate_all_rules(transactions, metric='lift')\n",
        "\n",
        "    te = TransactionEncoder()\n",
        "    encoded_matrix = te.fit_transform(transactions)\n",
        "    encoded_df = pd.DataFrame(encoded_matrix, columns=te.columns_)\n",
        "    cosine_sim = cosine_similarity(encoded_df.T)\n",
        "\n",
        "    test_item = 'milk'  # change this to any item in your dataset\n",
        "    item_based_recommendation(test_item, encoded_df, cosine_sim)\n",
        "    popularity_based_recommendation(encoded_df)\n",
        "    association_based_recommendation(test_item, best_rules)\n",
        "    hybrid_recommendation(test_item, encoded_df, cosine_sim, best_rules)\n",
        "\n",
        "    print(Fore.CYAN + \"\\nüìä Apriori Grid Search Results:\")\n",
        "    print(apriori_results.head())\n",
        "\n",
        "    print(Fore.CYAN + \"\\nüìä FP-Growth Grid Search Results:\")\n",
        "    print(fp_results.head())"
      ],
      "metadata": {
        "id": "17l9RNjGqZIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7e7d82-5433-4c29-9641-0ceb736421fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully loaded 519910 transactions\n",
            "\n",
            "üîç Performing Grid Search for APRIORI (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:34<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No valid rules found for APRIORI with current support/confidence ranges.\n",
            "\n",
            "üèÜ Best Apriori Parameters:\n",
            "{}\n",
            "\n",
            "üîç Performing Grid Search for FPGROWTH (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from pyECLAT import ECLAT\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "        self.metric_values = []  # To track metric values over iterations\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        \"\"\"Evaluate rules using selected metric\"\"\"\n",
        "        if rules.empty or self.metric not in rules.columns:\n",
        "            return 0\n",
        "        metric_value = rules[self.metric].mean()\n",
        "        self.metric_values.append(metric_value)  # Store the current metric value\n",
        "        return metric_value\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='apriori',\n",
        "                    support_range=np.arange(0.1, 0.5, 0.1),\n",
        "                    confidence_range=np.arange(0.5, 0.9, 0.1)):\n",
        "        \"\"\"Automated grid search for optimal support and confidence\"\"\"\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                        total=len(support_range) * len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        # Update the best score and parameters\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "\n",
        "                        # Display the updated metric after each iteration\n",
        "                        print(Fore.GREEN + f\"Updated {self.metric} after support={support}, confidence={confidence}: {score}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(Fore.RED + f\"Error with support {support} and confidence {confidence}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if results:\n",
        "            return pd.DataFrame(results).sort_values('score', ascending=False)\n",
        "        else:\n",
        "            print(Fore.YELLOW + f\"No valid rules found for {algorithm.upper()} with current support/confidence ranges.\")\n",
        "            return pd.DataFrame(columns=['support', 'confidence', 'score', 'num_rules'])\n",
        "\n",
        "    def get_metric_values(self):\n",
        "        \"\"\"Returns all recorded metric values\"\"\"\n",
        "        return self.metric_values\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    \"\"\"Prepare transaction data from DataFrame\"\"\"\n",
        "    transactions = df[items_column].dropna().apply(\n",
        "        lambda x: [item.strip() for item in str(x).split(',')]\n",
        "    )\n",
        "    print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "    return transactions.tolist()\n",
        "\n",
        "def item_based_recommendation(item, encoded_df, cosine_sim, top_n=5):\n",
        "    \"\"\"Generate recommendations based on item similarity\"\"\"\n",
        "    try:\n",
        "        item_index = list(encoded_df.columns).index(item)\n",
        "        sim_scores = list(enumerate(cosine_sim[item_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:top_n + 1]  # Exclude self\n",
        "        recommendations = [encoded_df.columns[i] for i, _ in sim_scores]\n",
        "        print(Fore.GREEN + f\"\\nüéØ Item-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    except ValueError:\n",
        "        print(Fore.RED + f\"\\n‚ö†Ô∏è Item '{item}' not found in dataset\")\n",
        "        return []\n",
        "\n",
        "def popularity_based_recommendation(encoded_df, top_n=5):\n",
        "    \"\"\"Generate recommendations based on overall popularity\"\"\"\n",
        "    item_counts = encoded_df.sum().sort_values(ascending=False)\n",
        "    popular_items = list(item_counts.head(top_n).index)\n",
        "    print(Fore.GREEN + f\"\\nüèÜ Popularity-based recommendations: {popular_items}\")\n",
        "    return popular_items\n",
        "\n",
        "def association_based_recommendation(item, rules, top_n=5):\n",
        "    \"\"\"Generate recommendations based on association rules\"\"\"\n",
        "    if 'antecedents' not in rules.columns or 'consequents' not in rules.columns:\n",
        "        print(Fore.YELLOW + \"‚ö†Ô∏è No valid association rules available.\")\n",
        "        return []\n",
        "\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item in x)]\n",
        "    if not item_rules.empty:\n",
        "        item_rules = item_rules.sort_values('lift', ascending=False)\n",
        "        recommendations = list(set().union(*item_rules.head(top_n)['consequents'].apply(list)))\n",
        "        print(Fore.GREEN + f\"\\nüîó Association-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(Fore.YELLOW + f\"\\n‚ÑπÔ∏è No association rules found for '{item}'\")\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(item, encoded_df, cosine_sim, rules, top_n=5):\n",
        "    \"\"\"Combine multiple recommendation approaches\"\"\"\n",
        "    item_based = item_based_recommendation(item, encoded_df, cosine_sim, top_n)\n",
        "    assoc_based = association_based_recommendation(item, rules, top_n)\n",
        "    popular = popularity_based_recommendation(encoded_df, top_n)\n",
        "\n",
        "    combined = list(set(item_based + assoc_based + popular))\n",
        "    combined = [i for i in combined if i != item]\n",
        "    print(Fore.BLUE + f\"\\n‚ú® Hybrid recommendations for '{item}': {combined[:top_n]}\")\n",
        "    return combined[:top_n]\n",
        "\n",
        "def generate_all_rules(transactions, metric='lift'):\n",
        "    \"\"\"Generate rules using all algorithms with optimal parameters\"\"\"\n",
        "    miner = AssociationRuleMiner(metric=metric)\n",
        "\n",
        "    # Apriori\n",
        "    apriori_results = miner.grid_search(transactions, algorithm='apriori')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best Apriori Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # FP-Growth\n",
        "    fp_results = miner.grid_search(transactions, algorithm='fpgrowth')\n",
        "    print(Fore.GREEN + \"\\nüèÜ Best FP-Growth Parameters:\")\n",
        "    print(miner.best_params)\n",
        "\n",
        "    # ECLAT\n",
        "    best_rules = miner.best_rules.copy()\n",
        "    try:\n",
        "        eclat = ECLAT(data=pd.DataFrame({'Transactions': {i: t for i, t in enumerate(transactions)}}))\n",
        "        _, eclat_rules = eclat.fit(min_support=miner.best_params.get('support', 0.1))\n",
        "\n",
        "        eclat_formatted = []\n",
        "        for itemset, support in eclat_rules.items():\n",
        "            if len(itemset) >= 2:\n",
        "                for i in range(1, len(itemset)):\n",
        "                    eclat_formatted.append({\n",
        "                        'antecedents': frozenset(itemset[:i]),\n",
        "                        'consequents': frozenset(itemset[i:]),\n",
        "                        'support': support,\n",
        "                        'confidence': None,\n",
        "                        'lift': None\n",
        "                    })\n",
        "\n",
        "        eclat_df = pd.DataFrame(eclat_formatted)\n",
        "        best_rules = pd.concat([best_rules, eclat_df], ignore_index=True)\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"ECLAT Error: {str(e)}\")\n",
        "\n",
        "    return best_rules, apriori_results, fp_results\n",
        "\n",
        "# ===== MAIN EXECUTION =====\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure your DataFrame 'df' is loaded and has 'Itemname' column\n",
        "    if 'Itemname' not in df.columns:\n",
        "        raise ValueError(\"‚ùå Column 'Itemname' not found in your DataFrame.\")\n",
        "\n",
        "    transactions = load_and_prepare_data(df, items_column='Itemname')\n",
        "\n",
        "    # Call the generate_all_rules function\n",
        "    best_rules, apriori_results, fp_results = generate_all_rules(transactions, metric='lift')\n",
        "\n",
        "    # Print the Apriori Grid Search Results\n",
        "    print(Fore.CYAN + \"\\nüìä Apriori Grid Search Results:\")\n",
        "    print(apriori_results.head())\n",
        "\n",
        "    # Show all the evaluated metric values during grid search\n",
        "    print(Fore.CYAN + \"\\nüî¢ Evaluation Metric Values (lift):\")\n",
        "    print(miner.get_metric_values())\n",
        "\n",
        "    # Generate encoded matrix and cosine similarity\n",
        "    te = TransactionEncoder()\n",
        "    encoded_matrix = te.fit_transform(transactions)\n",
        "    encoded_df = pd.DataFrame(encoded_matrix, columns=te.columns_)\n",
        "    cosine_sim = cosine_similarity(encoded_df.T)\n",
        "\n",
        "    # Define test item and generate recommendations\n",
        "    test_item = 'milk'  # change this to any item in your dataset\n",
        "    item_based_recommendation(test_item, encoded_df, cosine_sim)\n",
        "    popularity_based_recommendation(encoded_df)\n",
        "    association_based_recommendation(test_item, best_rules)\n",
        "    hybrid_recommendation(test_item, encoded_df, cosine_sim, best_rules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm_X98U6G1f1",
        "outputId": "d394a2d7-bfa2-49e5-e5ba-cfef2a7fc813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully loaded 519910 transactions\n",
            "\n",
            "üîç Performing Grid Search for APRIORI (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:34<00:00,  2.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No valid rules found for APRIORI with current support/confidence ranges.\n",
            "\n",
            "üèÜ Best Apriori Parameters:\n",
            "{}\n",
            "\n",
            "üîç Performing Grid Search for FPGROWTH (optimizing lift)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from itertools import product\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        if rules.empty or self.metric not in rules.columns:\n",
        "            return 0\n",
        "        return rules[self.metric].mean()\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='apriori',\n",
        "                   support_range=np.arange(0.01, 0.1, 0.02),\n",
        "                   confidence_range=np.arange(0.3, 0.8, 0.1)):\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "\n",
        "        # Sample a subset if dataset is too large\n",
        "        if len(transactions) > 10000:\n",
        "            sample_transactions = np.random.choice(transactions, size=10000, replace=False)\n",
        "            te_ary = te.fit_transform(sample_transactions)\n",
        "            df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "            print(Fore.YELLOW + \"‚ö†Ô∏è Using 10,000 transaction sample for performance\")\n",
        "\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                      total=len(support_range)*len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True, max_len=4)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True, max_len=4)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if results:\n",
        "            return pd.DataFrame(results).sort_values('score', ascending=False)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    try:\n",
        "        transactions = df[items_column].dropna().apply(\n",
        "            lambda x: [item.strip() for item in str(x).split(',')]\n",
        "        )\n",
        "        print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "        return transactions.tolist()\n",
        "    except KeyError:\n",
        "        available_cols = df.columns.tolist()\n",
        "        raise ValueError(f\"Column '{items_column}' not found. Available columns: {available_cols}\")\n",
        "\n",
        "# Main execution\n",
        "\n",
        "\n",
        "    # Prepare data\n",
        "    transactions = load_and_prepare_data(df, items_column='Itemname')\n",
        "\n",
        "    # Initialize miner\n",
        "    miner = AssociationRuleMiner(metric='lift')\n",
        "\n",
        "    # Run grid search with more appropriate parameters\n",
        "    print(Fore.BLUE + \"\\n‚öôÔ∏è Running with optimized parameters for large dataset...\")\n",
        "    results = miner.grid_search(\n",
        "        transactions,\n",
        "        algorithm='fpgrowth',\n",
        "        support_range=np.arange(0.005, 0.05, 0.005),\n",
        "        confidence_range=np.arange(0.2, 0.7, 0.1)\n",
        "    )\n",
        "\n",
        "    if not results.empty:\n",
        "        print(Fore.GREEN + \"\\nüèÜ Best Parameters:\")\n",
        "        print(miner.best_params)\n",
        "        print(Fore.CYAN + \"\\nTop Rules:\")\n",
        "        print(miner.best_rules.head())\n",
        "    else:\n",
        "        print(Fore.RED + \"\\n‚ö†Ô∏è No rules found. Try lowering support/confidence thresholds further.\")"
      ],
      "metadata": {
        "id": "7AnQw1pJPRnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from itertools import product\n",
        "from collections import defaultdict\n",
        "from colorama import Fore, init\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "init(autoreset=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class AssociationRuleMiner:\n",
        "    def __init__(self, metric='lift'):\n",
        "        self.metric = metric\n",
        "        self.best_params = {}\n",
        "        self.best_rules = pd.DataFrame()\n",
        "        self.metric_values = []\n",
        "\n",
        "    def evaluate_rules(self, rules):\n",
        "        \"\"\"Evaluate rules using selected metric\"\"\"\n",
        "        if rules.empty or self.metric not in rules.columns:\n",
        "            return 0\n",
        "        metric_value = rules[self.metric].mean()\n",
        "        self.metric_values.append(metric_value)\n",
        "        return metric_value\n",
        "\n",
        "    def grid_search(self, transactions, algorithm='fpgrowth',\n",
        "                   support_range=np.arange(0.005, 0.051, 0.005),\n",
        "                   confidence_range=np.arange(0.2, 0.7, 0.1),\n",
        "                   sample_size=20000):\n",
        "        \"\"\"Optimized grid search for large datasets\"\"\"\n",
        "        # Sample transactions if dataset is large\n",
        "        if len(transactions) > sample_size:\n",
        "            print(Fore.YELLOW + f\"‚ö†Ô∏è Sampling {sample_size} transactions for performance\")\n",
        "            transactions = np.random.choice(transactions, size=sample_size, replace=False)\n",
        "\n",
        "        te = TransactionEncoder()\n",
        "        te_ary = te.fit_transform(transactions)\n",
        "        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "        best_score = -1\n",
        "        results = []\n",
        "\n",
        "        print(Fore.CYAN + f\"\\nüîç Performing Grid Search for {algorithm.upper()} (optimizing {self.metric})...\")\n",
        "\n",
        "        for support, confidence in tqdm(product(support_range, confidence_range),\n",
        "                                      total=len(support_range)*len(confidence_range)):\n",
        "            try:\n",
        "                if algorithm == 'apriori':\n",
        "                    freq_items = apriori(df_encoded, min_support=support, use_colnames=True, max_len=4)\n",
        "                elif algorithm == 'fpgrowth':\n",
        "                    freq_items = fpgrowth(df_encoded, min_support=support, use_colnames=True, max_len=4)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if not freq_items.empty:\n",
        "                    rules = association_rules(freq_items, metric=\"confidence\", min_threshold=confidence)\n",
        "                    if not rules.empty:\n",
        "                        score = self.evaluate_rules(rules)\n",
        "                        results.append({\n",
        "                            'support': support,\n",
        "                            'confidence': confidence,\n",
        "                            'score': score,\n",
        "                            'num_rules': len(rules)\n",
        "                        })\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            self.best_params = {\n",
        "                                'support': support,\n",
        "                                'confidence': confidence,\n",
        "                                'algorithm': algorithm\n",
        "                            }\n",
        "                            self.best_rules = rules\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if results:\n",
        "            return pd.DataFrame(results).sort_values('score', ascending=False)\n",
        "        print(Fore.YELLOW + f\"No valid rules found for {algorithm.upper()} with current parameters\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    def get_metric_values(self):\n",
        "        return self.metric_values\n",
        "\n",
        "def load_and_prepare_data(df, items_column='Itemname'):\n",
        "    \"\"\"Prepare transaction data with robust column handling\"\"\"\n",
        "    try:\n",
        "        # Try to find the items column if exact name doesn't exist\n",
        "        if items_column not in df.columns:\n",
        "            for col in df.columns:\n",
        "                if 'item' in col.lower():\n",
        "                    items_column = col\n",
        "                    print(Fore.YELLOW + f\"‚ö†Ô∏è Using column '{col}' as items column\")\n",
        "                    break\n",
        "\n",
        "        transactions = df[items_column].dropna().apply(\n",
        "            lambda x: [item.strip().lower() for item in str(x).split(',') if item.strip()]\n",
        "        )\n",
        "        print(f\"‚úÖ Successfully loaded {len(transactions)} transactions\")\n",
        "        return transactions.tolist()\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"Error preparing data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def item_based_recommendation(item, encoded_df, cosine_sim, top_n=5):\n",
        "    \"\"\"Generate recommendations based on item similarity\"\"\"\n",
        "    try:\n",
        "        item = item.lower()\n",
        "        item_index = list(encoded_df.columns).index(item)\n",
        "        sim_scores = list(enumerate(cosine_sim[item_index]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:top_n+1]  # Exclude self\n",
        "        recommendations = [encoded_df.columns[i] for i, _ in sim_scores]\n",
        "        print(Fore.GREEN + f\"\\nüéØ Item-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    except ValueError:\n",
        "        print(Fore.RED + f\"\\n‚ö†Ô∏è Item '{item}' not found in dataset\")\n",
        "        return []\n",
        "\n",
        "def popularity_based_recommendation(encoded_df, top_n=5):\n",
        "    \"\"\"Generate recommendations based on overall popularity\"\"\"\n",
        "    item_counts = encoded_df.sum().sort_values(ascending=False)\n",
        "    popular_items = list(item_counts.head(top_n).index)\n",
        "    print(Fore.GREEN + f\"\\nüèÜ Popularity-based recommendations: {popular_items}\")\n",
        "    return popular_items\n",
        "\n",
        "def association_based_recommendation(item, rules, top_n=5):\n",
        "    \"\"\"Generate recommendations based on association rules\"\"\"\n",
        "    if rules.empty or 'antecedents' not in rules.columns:\n",
        "        print(Fore.YELLOW + \"‚ö†Ô∏è No valid association rules available.\")\n",
        "        return []\n",
        "\n",
        "    item = item.lower()\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item in x)]\n",
        "    if not item_rules.empty:\n",
        "        item_rules = item_rules.sort_values('lift', ascending=False)\n",
        "        recommendations = list(set().union(*item_rules.head(top_n)['consequents'].apply(list)))\n",
        "        print(Fore.GREEN + f\"\\nüîó Association-based recommendations for '{item}': {recommendations}\")\n",
        "        return recommendations\n",
        "    else:\n",
        "        print(Fore.YELLOW + f\"\\n‚ÑπÔ∏è No association rules found for '{item}'\")\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(item, encoded_df, cosine_sim, rules, top_n=5):\n",
        "    \"\"\"Combine multiple recommendation approaches\"\"\"\n",
        "    item_based = item_based_recommendation(item, encoded_df, cosine_sim, top_n)\n",
        "    assoc_based = association_based_recommendation(item, rules, top_n)\n",
        "    popular = popularity_based_recommendation(encoded_df, top_n)\n",
        "\n",
        "    combined = list(set(item_based + assoc_based + popular))\n",
        "    combined = [i for i in combined if i != item.lower()]\n",
        "    print(Fore.BLUE + f\"\\n‚ú® Hybrid recommendations for '{item}': {combined[:top_n]}\")\n",
        "    return combined[:top_n]\n",
        "\n",
        "def generate_all_rules(transactions, metric='lift'):\n",
        "    \"\"\"Generate rules using optimized parameters\"\"\"\n",
        "    miner = AssociationRuleMiner(metric=metric)\n",
        "\n",
        "    # First try FP-Growth (faster for large datasets)\n",
        "    fp_results = miner.grid_search(transactions, algorithm='fpgrowth')\n",
        "\n",
        "    if not fp_results.empty:\n",
        "        print(Fore.GREEN + \"\\nüèÜ Best FP-Growth Parameters:\")\n",
        "        print(miner.best_params)\n",
        "    else:\n",
        "        # Fall back to Apriori if FP-Growth fails\n",
        "        print(Fore.YELLOW + \"\\n‚ö†Ô∏è Trying Apriori as fallback...\")\n",
        "        apriori_results = miner.grid_search(transactions, algorithm='apriori')\n",
        "        if not apriori_results.empty:\n",
        "            print(Fore.GREEN + \"\\nüèÜ Best Apriori Parameters:\")\n",
        "            print(miner.best_params)\n",
        "\n",
        "    return miner.best_rules, miner.best_params\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Load your data\n",
        "        if 'df' not in locals():\n",
        "            raise ValueError(\"Please load your DataFrame first\")\n",
        "\n",
        "        # Prepare data\n",
        "        transactions = load_and_prepare_data(df)\n",
        "\n",
        "        # Generate rules with automatic parameter tuning\n",
        "        best_rules, best_params = generate_all_rules(transactions)\n",
        "\n",
        "        if best_rules.empty:\n",
        "            print(Fore.RED + \"\\n‚ùå No rules generated. Try adjusting parameters or cleaning data.\")\n",
        "            return\n",
        "\n",
        "        # Prepare recommendation engine components\n",
        "        te = TransactionEncoder()\n",
        "        encoded_matrix = te.fit_transform(transactions)\n",
        "        encoded_df = pd.DataFrame(encoded_matrix, columns=te.columns_)\n",
        "        cosine_sim = cosine_similarity(encoded_df.T)\n",
        "\n",
        "        # Example recommendations\n",
        "        test_item = 'milk'  # Change this to test different items\n",
        "        hybrid_recommendation(test_item, encoded_df, cosine_sim, best_rules)\n",
        "\n",
        "        # Show top rules\n",
        "        print(Fore.CYAN + \"\\nüìä Top Association Rules:\")\n",
        "        print(best_rules.sort_values('lift', ascending=False).head(10))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"\\n‚ùå Error in main execution: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fMLHPpT0RHeN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}